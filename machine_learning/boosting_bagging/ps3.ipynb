{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9azx7rJyBOMp"
      },
      "source": [
        "# Problem Set 3"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gkEPkpDsBOMv"
      },
      "source": [
        "# Problem 3: Medical Diagnostics\n",
        "\n",
        "These data sets were generated using the UCI SPECT heart data set (follow the link for information about the format of the data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xmNJhBnBOMv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "heart_train_path = r'./heart_train.data'\n",
        "heart_test_path = r'./heart_test.data'\n",
        "train = np.loadtxt(heart_train_path, delimiter=\",\")\n",
        "test = np.loadtxt(heart_test_path, delimiter=\",\")\n",
        "\n",
        "X_train = train[:, 1:]\n",
        "y_train = train[:, 0]\n",
        "y_train[y_train == 0] = -1\n",
        "\n",
        "X_test = test[:, 1:]\n",
        "y_test = test[:, 0]\n",
        "y_test[y_test == 0] = -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zk2kIvrBOMw"
      },
      "source": [
        "## Suppose that the hypothesis space consists of all decision trees with exactly three attribute splits (repetition along the same path is allowed) for this data set."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hHXhT_8MBOMx"
      },
      "source": [
        "### Run the adaBoost algorithm for five rounds to train a classifier for this data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ0GRCS5BOMx"
      },
      "outputs": [],
      "source": [
        "# Functions to build the hypothesis space\n",
        "\n",
        "class DecisionTree():\n",
        "    def __init__(self, root):\n",
        "        self.root = root\n",
        "    def __predict(self, root, x):\n",
        "        if root.label is not None:\n",
        "            return root.label\n",
        "        return self.__predict(root.children[x[root.attr]], x)    \n",
        "    def predict(self, x):\n",
        "        if self.root.label is not None:\n",
        "            return self.root.label\n",
        "        return self.__predict(self.root.children[x[self.root.attr]], x)\n",
        "\n",
        "\n",
        "class Node():\n",
        "    def __init__(self, attr, children = None, label = None):\n",
        "        self.attr = attr\n",
        "        self.children = children\n",
        "        self.label = label\n",
        "\n",
        "# Recursively builds all possible trees given three attributes\n",
        "# Children = list of attribute indicies that will be children of the current root\n",
        "# attr = index of the attribute of the current node\n",
        "# Returns a list of root nodes for each tree\n",
        "def build3split(children, attr):\n",
        "    trees = []\n",
        "\n",
        "    # Just leafs\n",
        "    if len(children) == 0:\n",
        "        for left_value in [-1, 1]:\n",
        "            for right_value in [-1, 1]:\n",
        "                left_leaf = Node(attr, label=left_value)\n",
        "                right_leaf = Node(attr, label=right_value)\n",
        "                trees.append(Node(attr, children={0 : left_leaf, 1 : right_leaf}))\n",
        "        return trees\n",
        "\n",
        "    # Single Child Node\n",
        "    single_subtrees = build3split(children[1:], children[0])\n",
        "    for subtree in single_subtrees:\n",
        "        for leaf_value in [-1, 1]:\n",
        "            leaf = Node(attr, label=leaf_value)\n",
        "            trees.append(Node(attr, children={0 : subtree, 1 : leaf}))\n",
        "            trees.append(Node(attr, children={0 : leaf, 1 : subtree}))\n",
        "\n",
        "    # Two Child Node\n",
        "    if (len(children) >= 2):\n",
        "        left_subtrees = build3split(children[2:], children[0])\n",
        "        right_subtrees = build3split(children[2:], children[1])\n",
        "\n",
        "        for left_tree in left_subtrees:\n",
        "            for right_tree in right_subtrees:\n",
        "                trees.append(Node(attr, children={0 : left_tree, 1 : right_tree}))\n",
        "\n",
        "    return trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swGDwgRWBOMy"
      },
      "outputs": [],
      "source": [
        "# Functions to implement AdaBoosting\n",
        "\n",
        "# Prints a tree given the root node\n",
        "def print_tree(root, prefix=\"├──\"):\n",
        "    if root is None:\n",
        "        return\n",
        "    if root.label is not None:\n",
        "        print(f\"{root.label}\")\n",
        "        return\n",
        "    print(f\"Split on index: {root.attr}\")\n",
        "    for attr, child in root.children.items():\n",
        "        print(f\"\\t{prefix}Attr: {attr} => \", end=\"\")\n",
        "        print_tree(child, \"\\t\"+ prefix )\n",
        "    return\n",
        "\n",
        "# Calculates the accuracy of a ensemble of hypotheses in boosting\n",
        "# ensemble = list of tuples (alpha, model)\n",
        "def ensemble_accuracy(ensemble, X, y):\n",
        "    result = []\n",
        "    for x in X:\n",
        "        y_pred = 0\n",
        "        for alpha, model in ensemble:\n",
        "            y_pred += alpha * model.predict(x)\n",
        "        result.append(y_pred)\n",
        "    result = np.sign(result)\n",
        "    return np.sum(np.equal(result, y)) / len(y)\n",
        "\n",
        "# Perform Adaboosting on a given hypothesis space for fixed iterations\n",
        "# hypo_space = list of possible hypotheses\n",
        "# iter = number of boosting iterations to perform\n",
        "# train = (X_train, y_train)\n",
        "# test = (X_test, y_test)\n",
        "# Returns (ensemble list, training accuracy list, test accuracy list)\n",
        "def adaboost(hypo_space, iter, train, test):\n",
        "    X_train = train[0]\n",
        "    y_train = train[1]\n",
        "    weights = np.array([1 / len(y_train)] * len(y_train)) # Initalize weights\n",
        "    ensemble = [] # List of (alphas, models) for each iteration\n",
        "\n",
        "    # list of arrays of predicted y's for each hypothesis\n",
        "    hypo_predictions =[np.array([hypo.predict(x) for x in X_train]) for hypo in hypo_space]\n",
        "    \n",
        "    # 2-d array where each row is a boolean array if the prediction is wrong\n",
        "    error_masks = np.array([predictions != y_train for predictions in hypo_predictions])\n",
        "\n",
        "    # accuracy per iteration\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "\n",
        "    # Perform boosting for n iterations\n",
        "    for i in range(iter):\n",
        "\n",
        "        # Calculate error for each possible hypothesis\n",
        "        # [np.sum(mask * weights) for mask in error_masks] \n",
        "        weighted_errors = np.dot(error_masks, weights)\n",
        "        index_min = np.argmin(weighted_errors)\n",
        "        \n",
        "        best_error = weighted_errors[index_min]\n",
        "        best_hypo = hypo_space[index_min]\n",
        "        best_predictions = hypo_predictions[index_min]\n",
        "\n",
        "\n",
        "        # Calculate h(x) for new weights\n",
        "        y_pred = best_predictions\n",
        "\n",
        "        # Calucate alpha\n",
        "        alpha = 0.5 * np.log((1 - best_error) / best_error)\n",
        "\n",
        "        # Update weights\n",
        "        weights = (weights * np.exp(-y_train * y_pred * alpha)) / (2 * np.sqrt(best_error * (1 - best_error)))\n",
        "        ensemble.append((alpha, best_hypo))\n",
        "        \n",
        "        # Calculate Accuracy\n",
        "        train_acc.append(ensemble_accuracy(ensemble, train[0], train[1]))\n",
        "        test_acc.append(ensemble_accuracy(ensemble, test[0], test[1]))\n",
        "\n",
        "        # Print metrics for first five iterations\n",
        "        if i < 5:\n",
        "            print(f\"Iteration: {i}\\tError:{best_error}\\tAlpha:{alpha}\")\n",
        "            print_tree(best_hypo.root)\n",
        "\n",
        "    return ensemble, train_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71CeN4ZZBOMz",
        "outputId": "ed853737-a61c-4a38-bf11-e6b7e90fecfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "851840\n",
            "Iteration: 0\tError:0.1875\tAlpha:0.7331685343967135\n",
            "Split on index: 7\n",
            "\t├──Attr: 0 => Split on index: 10\n",
            "\t\t├──Attr: 0 => Split on index: 15\n",
            "\t\t\t├──Attr: 0 => -1\n",
            "\t\t\t├──Attr: 1 => 1\n",
            "\t\t├──Attr: 1 => 1\n",
            "\t├──Attr: 1 => 1\n",
            "Iteration: 1\tError:0.2692307692307692\tAlpha:0.4992644150555637\n",
            "Split on index: 11\n",
            "\t├──Attr: 0 => Split on index: 19\n",
            "\t\t├──Attr: 0 => Split on index: 21\n",
            "\t\t\t├──Attr: 0 => -1\n",
            "\t\t\t├──Attr: 1 => 1\n",
            "\t\t├──Attr: 1 => 1\n",
            "\t├──Attr: 1 => 1\n",
            "Iteration: 2\tError:0.34035087719298246\tAlpha:0.33086549216328326\n",
            "Split on index: 12\n",
            "\t├──Attr: 0 => Split on index: 2\n",
            "\t\t├──Attr: 0 => Split on index: 21\n",
            "\t\t\t├──Attr: 0 => 1\n",
            "\t\t\t├──Attr: 1 => -1\n",
            "\t\t├──Attr: 1 => -1\n",
            "\t├──Attr: 1 => 1\n",
            "Iteration: 3\tError:0.33719604863221886\tAlpha:0.33790736963862894\n",
            "Split on index: 3\n",
            "\t├──Attr: 0 => Split on index: 21\n",
            "\t\t├──Attr: 0 => -1\n",
            "\t\t├──Attr: 1 => 1\n",
            "\t├──Attr: 1 => Split on index: 8\n",
            "\t\t├──Attr: 0 => -1\n",
            "\t\t├──Attr: 1 => 1\n",
            "Iteration: 4\tError:0.36971122376230825\tAlpha:0.2667279323771729\n",
            "Split on index: 4\n",
            "\t├──Attr: 0 => 1\n",
            "\t├──Attr: 1 => Split on index: 12\n",
            "\t\t├──Attr: 0 => Split on index: 18\n",
            "\t\t\t├──Attr: 0 => -1\n",
            "\t\t\t├──Attr: 1 => 1\n",
            "\t\t├──Attr: 1 => 1\n"
          ]
        }
      ],
      "source": [
        "# Builds the hypothesis space\n",
        "hypo_roots = []\n",
        "# Iterates through all possible attributes for each split\n",
        "for i in range(len(X_train[0])):\n",
        "    for j in range(len(X_train[0])):\n",
        "        for k in range(len(X_train[0])):\n",
        "            hypo_roots += build3split([j, k], i)\n",
        "\n",
        "# Encapsulates the nodes into a DecisionTree\n",
        "hypo_space = [DecisionTree(root) for root in hypo_roots]\n",
        "print(len(hypo_space))\n",
        "\n",
        "# Performs Adaboost\n",
        "ensemble, train_acc, test_acc = adaboost(hypo_space, 10, (X_train, y_train), (X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD1XO-98D_Hp"
      },
      "source": [
        "![trees](https://drive.google.com/uc?export=download&id=1DE3XPpgmxM2sIH1ZXxbStwx3_PCG4hue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCS3sjm_BOM0"
      },
      "source": [
        "### (b) Run the adaBoost algorithm for 10 rounds of boosting. Plot the accuracy on the training and test sets versus iteration number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "rQfuYFJ1BOMz",
        "outputId": "d107cb9e-31f8-4f01-b923-cd013daa0956"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f247fd02dd0>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(test_acc, label=\"Test\")\n",
        "plt.plot(train_acc, label=\"Train\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_BDZligBOM0"
      },
      "source": [
        "## 2. Now, suppose that the hypothesis space consists of only height 1 decision trees for this dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RUMx4NdBOM0"
      },
      "outputs": [],
      "source": [
        "# Construct the hypothesis space\n",
        "\n",
        "class DecisionTreeSingle():\n",
        "    def __init__(self, attr, result):\n",
        "        self.root = None\n",
        "        self.attr = attr\n",
        "        self.result = result\n",
        "        return\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        # Return scalar\n",
        "        if (len(X.shape) == 1):\n",
        "            return self.result.get(X[self.attr])\n",
        "\n",
        "        # Return array\n",
        "        prediction = []\n",
        "        for x in X:\n",
        "            prediction.append(self.result.get(x[self.attr]))\n",
        "        return np.array(prediction)\n",
        "\n",
        "hypothesis = []\n",
        "\n",
        "hypothesis.append(DecisionTreeSingle(0, {0:1, 1:1}))\n",
        "hypothesis.append(DecisionTreeSingle(0, {0:-1, 1:-1}))\n",
        "\n",
        "for i in range(X_train.shape[1]):\n",
        "    split = {0:1, 1:-1}\n",
        "    hypothesis.append(DecisionTreeSingle(i, split))\n",
        "    split = {0:-1, 1:1}\n",
        "    hypothesis.append(DecisionTreeSingle(i, split))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-TrvZDpBOM0"
      },
      "source": [
        "### (a) Use coordinate descent to minimize the exponential loss function for this hypothesis space over the training set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nas5SRp4BOM1",
        "outputId": "d9218b8d-8d26-4314-a2f2-f84763e5a269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "alphas: [16.598369648291165, 0, 5.238805971631234, 0, 0.47878595992474526, 0, 10.472188359285479, 0, -0.8216251727672549, 0, -0.3193447614291511, 0, -0.1258081462271163, 0, -0.6069409655252586, 0, -10.074099991037347, 0, 8.746769277834257, 0, -5.104734566463085, 0, -0.39314794125167296, 0, 0.4578327302097394, 0, -0.7928826757807175, 0, -8.20961695410614, 0, -3.79772439748582, 0, -0.7405527269504613, 0, -6.565002971381626, 0, -4.712892170871722, 0, -0.050552423450202735, 0, -0.33133883633933053, 0, -0.16043007466846937, 0, -0.38306226436183455, 0]\n",
            "exponential loss: 39.33070695439794\n"
          ]
        }
      ],
      "source": [
        "# Initial alphas = 0's\n",
        "# Stopping condition: when the max change in alpha between a pass through all \n",
        "# the hypotheses is less than 1e-5\n",
        "\n",
        "# Any alphas that are 1e-10 away from 0 are truncated to exactly 0\n",
        "# 1e-5 takes around 15 mins while 1e-4 takes around 2 mins\n",
        "# The final alphas would be different but the test accuracy is the exact same\n",
        "\n",
        "def coord_descent(hypothesis, X_train, y_train):\n",
        "    old_alphas = [0] * len(hypothesis)\n",
        "    alphas = [0] * len(hypothesis)\n",
        "    iter_diff = 1\n",
        "    predictions = [hypo.predict(X_train) for hypo in hypothesis]    # predicted y's for each hypothesis\n",
        "    correct_masks = [prediction == y_train for prediction in predictions]   # boolean masks of correct predicted y's and true y's \n",
        "\n",
        "    while iter_diff > 1e-5:\n",
        "\n",
        "        # A round of going through all the hypotheses to calculate new alpha\n",
        "        for i, correct_mask in enumerate(correct_masks):\n",
        "            # Instead of directly iterating through each hypothesis\n",
        "            # We iterate through the arrays of correct/incorrect boolean masks\n",
        "            # This cacheing step saves from repredicting X_train and is equivalent to:\n",
        "            # correct_mask = hypo.predict(X_train) == y_train\n",
        "            # 1. sum_{t != t'} a_t h_t(x^m)\n",
        "            hypo_summation = np.zeros(len(y_train))\n",
        "\n",
        "            for j, prediction in enumerate(predictions):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if alphas[j] == 0:\n",
        "                    continue\n",
        "                hypo_summation += alphas[j] * prediction\n",
        "\n",
        "            # 2. exp(-y^m times summation)\n",
        "            exp_summation = np.exp(-y_train * hypo_summation)\n",
        "                    \n",
        "            # 3. Sum up all correctly and incorrectly classified points separately\n",
        "            correct_sum = np.sum(exp_summation[correct_mask])\n",
        "            incorrect_sum = np.sum(exp_summation[np.invert(correct_mask)])\n",
        "\n",
        "            # a_t = 1/2 ln (correct / incorrect)\n",
        "            new_alpha = 0.5 * math.log(correct_sum / incorrect_sum)\n",
        "            new_alpha = new_alpha if new_alpha > 1e-10 or new_alpha < -1e-10 else 0\n",
        "            alphas[i] = new_alpha\n",
        "\n",
        "        iter_diff = max([abs(x - y) for x, y in zip(alphas, old_alphas)])\n",
        "        old_alphas = alphas.copy()\n",
        "\n",
        "    # Calculate loss\n",
        "    hypo_summation = np.zeros(len(y_train))\n",
        "    for j, prediction in enumerate(predictions):\n",
        "        if alphas[j] == 0:\n",
        "          continue\n",
        "        hypo_summation += alphas[j] * prediction\n",
        "    exp_loss = np.sum(np.exp(-y_train * hypo_summation))\n",
        "\n",
        "    return alphas, exp_loss\n",
        "\n",
        "alphas, loss = coord_descent(hypothesis, X_train, y_train)\n",
        "print(f\"alphas: {alphas}\")\n",
        "print(f\"exponential loss: {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R82usZy6BOM2"
      },
      "outputs": [],
      "source": [
        "# for 1e-5, run this if you want to skip coord descent\n",
        "alphas = [16.598369648291165, 0, 5.238805971631234, 0, 0.47878595992474526, 0, 10.472188359285479, 0, -0.8216251727672549, 0, -0.3193447614291511, 0, -0.1258081462271163, 0, -0.6069409655252586, 0, -10.074099991037347, 0, 8.746769277834257, 0, -5.104734566463085, 0, -0.39314794125167296, 0, 0.4578327302097394, 0, -0.7928826757807175, 0, -8.20961695410614, 0, -3.79772439748582, 0, -0.7405527269504613, 0, -6.565002971381626, 0, -4.712892170871722, 0, -0.050552423450202735, 0, -0.33133883633933053, 0, -0.16043007466846937, 0, -0.38306226436183455, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFI0BGSCBOM2"
      },
      "source": [
        "### (b) What is the accuracy of the resulting classifier on the test data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS3M9uJcBOM2",
        "outputId": "7ac14272-9590-492e-c634-c3262650817b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test acc = 0.679144385026738\n"
          ]
        }
      ],
      "source": [
        "ensemble = [(alpha, model) for alpha, model in zip(alphas, hypothesis)]\n",
        "test_acc = ensemble_accuracy(ensemble, X_test, y_test)\n",
        "\n",
        "print(f\"test acc = {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnT7ieANBOM2"
      },
      "source": [
        "### (c) What is the accuracy of adaBoost after 20 rounds for this hypothesis space on the test data? How does the α learned by adaBoost compare to the one learned by gradient descent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NqPKELuANDA"
      },
      "source": [
        "The alphas in adaboost are restricted to only 20 values and are postive real numbers. In contrast, gradient descent had alphas for all hypotheses and can be any real number (i.e. 0 or negative as well). Additionally, gradient descent have few hypotheses that have large alpha values while adaBoost's alphas are closer in value with each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAMrBrgTBOM2",
        "outputId": "062ac05f-1f9a-4585-d230-fe6d304c8f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 0\tError:0.27499999999999997\tAlpha:0.4847002785940519\n",
            "Iteration: 1\tError:0.38557993730407536\tAlpha:0.2329651519290496\n",
            "Iteration: 2\tError:0.35050087108013944\tAlpha:0.30841915161660405\n",
            "Iteration: 3\tError:0.368625044883102\tAlpha:0.2690599604033768\n",
            "Iteration: 4\tError:0.43680235707837023\tAlpha:0.12707489951194642\n",
            "alphas = [0.4847002785940519, 0.2329651519290496, 0.30841915161660405, 0.2690599604033768, 0.12707489951194642, 0.19391360519349726, 0.16646581973436553, 0.20484093019666014, 0.11233780854211324, 0.1691531043753745, 0.12721184797605253, 0.153863770691764, 0.09807054208811175, 0.15796706354327933, 0.13458519734012883, 0.13035666473731994, 0.10144574639261594, 0.12621641283923687, 0.1074496408797689, 0.11081674484994215]\n",
            "test acc = 0.6684491978609626\n"
          ]
        }
      ],
      "source": [
        "ensemble, train_acc, test_acc = adaboost(hypothesis, 20, (X_train, y_train), (X_test, y_test))\n",
        "print(f\"alphas = {[alpha for alpha, _ in ensemble]}\")\n",
        "print(f\"test acc = {test_acc[-1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr2xn_O0BOM2"
      },
      "source": [
        "### (d) Use bagging, with 20 bootstrap samples, to produce an average classifier for this data set. How does it compare to the previous classifiers in terms of accuracy on the test set?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHGzjwI_G-Dl"
      },
      "source": [
        "It has a lower accuracy on the test set compared to the other two methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1IUTmCcBOM2",
        "outputId": "4fd0e22b-eda8-4b66-cafb-cfd223b30b3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test acc = 0.6149732620320856\n"
          ]
        }
      ],
      "source": [
        "def bagging(hypo_space, size, X, y):\n",
        "    ensemble = []\n",
        "    for _ in range(size):\n",
        "        indicies = np.random.choice(len(y), len(y))\n",
        "        X_train = X[indicies]\n",
        "        y_train = y[indicies]\n",
        "\n",
        "        best_model = None\n",
        "        best_acc = 0\n",
        "\n",
        "        for hypo in hypo_space:\n",
        "            y_pred = hypo.predict(X_train)\n",
        "            acc = np.sum(np.equal(y_pred, y_train)) / len(y_train)\n",
        "            if acc > best_acc:\n",
        "                best_model = hypo\n",
        "                best_acc = acc\n",
        "        \n",
        "        ensemble.append((1/size, best_model))\n",
        "            \n",
        "    return ensemble\n",
        "\n",
        "ensemble = bagging(hypothesis, 20, X_train, y_train)\n",
        "test_acc = ensemble_accuracy(ensemble, X_test, y_test)\n",
        "print(f\"test acc = {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x51N5I2KBOM3"
      },
      "source": [
        "### (e) Which of these 3 methods should be preferred for this data set and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erq8DVngBOM3"
      },
      "source": [
        "Looking at the test accuracy, we would prefer coordinate descent because it has the heighest accuracy of the three models. However, for coordinate descent we would have to try all possible hypotheses for predication, it may be more computationally expensive compared to adaboosting where we only have to run 20 hypotheses. If the training data, is not representative of real world patient data, the two boosting methods may overfit to the training data and may not generalize well in practice. The bagging method has a slightly worse testing accuracy but may generalize better."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:51:29) [MSC v.1929 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "eee0ec1dfc014c5e818c9fd6187bf3227a654b7ffe32fcac85496f478f59351d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
