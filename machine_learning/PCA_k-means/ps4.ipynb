{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "233Z6TfZUtu2"
      },
      "source": [
        "# Problem Set 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ssR3bGMi2al"
      },
      "outputs": [],
      "source": [
        "# Boiler plate\n",
        "\n",
        "import numpy as np\n",
        "import cvxopt\n",
        "import math\n",
        "import random\n",
        "cvxopt.solvers.options['show_progress'] = False\n",
        "\n",
        "prostate_path = r'./prostate_GE.data'\n",
        "leaf_path = r'./leaf.data'\n",
        "prostate_data = np.loadtxt(prostate_path, delimiter=\",\")\n",
        "leaf_data = np.loadtxt(leaf_path, delimiter=\",\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GbSrZeZdMF2I"
      },
      "source": [
        "# Problem 1: PCA and Feature Selection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MYg-nUuWML7U"
      },
      "source": [
        "## 1.1 SVMs and PCA \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2cb4tY3i-Hl"
      },
      "outputs": [],
      "source": [
        "# Preprocess data\n",
        "# label: 2 -> -1\n",
        "\n",
        "prostate_data[prostate_data[:,-1] == 2, -1] = -1\n",
        "prostate_train = prostate_data[:80]\n",
        "prostate_test = prostate_data[80:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab4zQNMMWrQN"
      },
      "source": [
        "Looking at the eigen values, $k$ should be at most 80 because each data point can be described by a dimension, therefore more than 80 features will not be necessary for strictly describing the training data. However, this will overfit to the training sample. Looking at the top 6 eigen values, it appears that the top 3 values describes the data the most with eigen values $>1500$ and then it greatly decreases beyond the top 3. This implies $k=3$ should describe most of the data and perform adequately. In practice, $k$ should be as large as possible with a trade off between accuracy, generality, and computational cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xSEXPBlMsC0",
        "outputId": "929f3027-3dcf-4af2-a5c1-19afe01b5905"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank 0:\t21756.602960911554\n",
            "Rank 1:\t1730.7702745518977\n",
            "Rank 2:\t1570.9895436838829\n",
            "Rank 3:\t786.1252951581539\n",
            "Rank 4:\t629.218476538095\n",
            "Rank 5:\t615.1779637357916\n"
          ]
        }
      ],
      "source": [
        "class PCA():\n",
        "    def __init__(self):\n",
        "        self.eigenvalues = None\n",
        "        self.eigenvectors = None\n",
        "        self.M = None\n",
        "    def fit(self, X):\n",
        "        M = np.mean(X, axis=0)\n",
        "        W = X - M\n",
        "        COV = np.matmul(W.T, W)\n",
        "        self.eigenvalues, self.eigenvectors = np.linalg.eig(COV)\n",
        "        self.eigenvalues = np.real(self.eigenvalues)\n",
        "        self.eigenvectors = np.real(self.eigenvectors)\n",
        "        self.M = M\n",
        "    def transform(self, X, k):\n",
        "        return np.matmul(X - self.M, self.eigenvectors[:,:k])\n",
        "\n",
        "decomp = PCA()\n",
        "decomp.fit(prostate_train[:, :-1])\n",
        "\n",
        "for i, ev in enumerate(decomp.eigenvalues[:6]):\n",
        "    print(f\"Rank {i}:\\t{ev}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nqOMjNqcMrN9"
      },
      "source": [
        "* For each k ∈ {1, 2, 3, 4, 5, 6}, project the training data into the best k dimensional subspace\n",
        "(with respect to the Frobenius norm) and use the SVM with slack formulation to learn a\n",
        "classifier for each c ∈ {1, 10, 100, 1000}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6fUF_KzMVVm",
        "outputId": "6a32f10d-9491-469a-aa8a-5823e2671742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c: 1, k: 1\t average error: 0.36250000000000004\n",
            "c: 1, k: 2\t average error: 0.25\n",
            "c: 1, k: 3\t average error: 0.15000000000000002\n",
            "c: 1, k: 4\t average error: 0.09999999999999998\n",
            "c: 1, k: 5\t average error: 0.16249999999999998\n",
            "c: 1, k: 6\t average error: 0.19999999999999996\n",
            "c: 10, k: 1\t average error: 0.36250000000000004\n",
            "c: 10, k: 2\t average error: 0.25\n",
            "c: 10, k: 3\t average error: 0.15000000000000002\n",
            "c: 10, k: 4\t average error: 0.09999999999999998\n",
            "c: 10, k: 5\t average error: 0.16249999999999998\n",
            "c: 10, k: 6\t average error: 0.19999999999999996\n",
            "c: 100, k: 1\t average error: 0.36250000000000004\n",
            "c: 100, k: 2\t average error: 0.25\n",
            "c: 100, k: 3\t average error: 0.15000000000000002\n",
            "c: 100, k: 4\t average error: 0.09999999999999998\n",
            "c: 100, k: 5\t average error: 0.16249999999999998\n",
            "c: 100, k: 6\t average error: 0.19999999999999996\n",
            "c: 1000, k: 1\t average error: 0.36250000000000004\n",
            "c: 1000, k: 2\t average error: 0.25\n",
            "c: 1000, k: 3\t average error: 0.15000000000000002\n",
            "c: 1000, k: 4\t average error: 0.09999999999999998\n",
            "c: 1000, k: 5\t average error: 0.16249999999999998\n",
            "c: 1000, k: 6\t average error: 0.19999999999999996\n",
            "Best c : 1, best k : 4 with error: 0.09999999999999998\n"
          ]
        }
      ],
      "source": [
        "# Note: This directly solves the optimization problem without using kernels\n",
        "# Using the lagrangian formulation, the exact error differs slightly but \n",
        "# the resultant test accuracy and hyperparameter selections are nearly the same\n",
        "\n",
        "class LinearSlackSVM():\n",
        "    def __init__(self):\n",
        "        self.w = 0\n",
        "        self.b = 0\n",
        "    def fit(self, X, y, c):\n",
        "        num_data = X.shape[0] # num of data points\n",
        "        dim_data = X.shape[1]\n",
        "\n",
        "        # Compute P\n",
        "        P = np.identity(dim_data + 1 + num_data)\n",
        "        for i in range(dim_data, dim_data + 1 + num_data):\n",
        "            P[i, i] = 0\n",
        "        P = cvxopt.matrix(P)\n",
        "\n",
        "        # Compute q\n",
        "        q = np.zeros(dim_data + 1 + num_data)\n",
        "        for i in range(dim_data + 1, dim_data + 1 + num_data):\n",
        "            q[i] = c\n",
        "        q = cvxopt.matrix(q)\n",
        "\n",
        "        # Compute G\n",
        "        G = np.zeros((2 * num_data, dim_data + 1 + num_data))\n",
        "        for i, row in enumerate(X):\n",
        "            slack = np.zeros(num_data)\n",
        "            slack[i] = -1\n",
        "            temp = np.append(-y[i] * row[:], -y[i])\n",
        "            temp = np.append(temp, slack)\n",
        "            G[i] = temp\n",
        "\n",
        "        for i in range(num_data):\n",
        "            temp = np.zeros(dim_data + 1 + num_data)\n",
        "            temp[i + dim_data + 1] = -1\n",
        "            G[i + num_data] = temp\n",
        "        G = cvxopt.matrix(G)\n",
        "\n",
        "        # Compute h\n",
        "        h = cvxopt.matrix(np.append(-np.ones(num_data), np.zeros(num_data)))\n",
        "\n",
        "        solution = cvxopt.solvers.qp(P, q, G, h)\n",
        "        \n",
        "        w = np.array(solution['x']).flatten()[:dim_data]\n",
        "        b = np.array(solution['x']).flatten()[dim_data]\n",
        "        self.w = w\n",
        "        self.b = b\n",
        "\n",
        "        return (w, b)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)\n",
        "\n",
        "    def accuracy(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        correct = np.equal(y, y_pred)\n",
        "        return np.count_nonzero(correct) / len(y)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Since there are multiple c values that have the highest accuracy\n",
        "# we will arbitrarily pick the lowest c with the highest acc\n",
        "\n",
        "# Split training data into 10 folds\n",
        "folds = np.split(prostate_train, 10)\n",
        "best_c = 0\n",
        "best_acc = 0\n",
        "best_k = 0\n",
        "\n",
        "c_values = [1, 10, 100, 1000]\n",
        "k_values = [1, 2, 3, 4, 5, 6]\n",
        "for c in c_values:\n",
        "    for k in k_values:\n",
        "        avg_acc = 0\n",
        "        for i in range(len(folds)):\n",
        "            data = [x for j, x in enumerate(folds) if j != i] \n",
        "            data = np.concatenate(data)\n",
        "\n",
        "            X_train = decomp.transform(data[:, :-1], k)\n",
        "            y_train = data[:, -1]\n",
        "\n",
        "            X_val = decomp.transform(folds[i][:, :-1], k)\n",
        "            y_val = folds[i][:, -1]\n",
        "\n",
        "            model = LinearSlackSVM()\n",
        "            model.fit(X_train, y_train, c)\n",
        "\n",
        "            val_acc = model.accuracy(X_val, y_val)\n",
        "            avg_acc += val_acc\n",
        "\n",
        "        avg_acc /= len(folds)\n",
        "        if avg_acc > best_acc:\n",
        "            best_c = c\n",
        "            best_acc = avg_acc\n",
        "            best_k = k\n",
        "        print(f\"c: {c}, k: {k}\\t average error: {1 - avg_acc}\")\n",
        "\n",
        "print(f\"Best c : {best_c}, best k : {best_k} with error: {1 - best_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ssArp0tM6FG"
      },
      "source": [
        "* What is the performance you achieve on the test set via the proper hyperparameter selection\n",
        "procedure above?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZEqUzaSMGZ_",
        "outputId": "6b8ecc06-66a1-44ce-bd69-049105a46ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing accuracy: 0.7727272727272727\n"
          ]
        }
      ],
      "source": [
        "# Retrain on all training data\n",
        "X_train = decomp.transform(prostate_train[:, :-1], best_k)\n",
        "y_train = prostate_train[:, -1]\n",
        "\n",
        "model = LinearSlackSVM()\n",
        "model.fit(X_train, y_train, best_c)\n",
        "\n",
        "# Test on testing set\n",
        "X_test = decomp.transform(prostate_test[:, :-1], best_k)\n",
        "y_test = prostate_test[:, -1]\n",
        "test_acc = model.accuracy(X_test, y_test)\n",
        "\n",
        "print(f\"Testing accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afQ56zPVM7s3"
      },
      "source": [
        "* Now suppose that we don’t do proper hyperparameter selection. What is the best performance\n",
        "that you can achieve on the test set if you tune the hyperparameters using the test set instead\n",
        "of the validation set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7chSW1nM_I4",
        "outputId": "d5ec3f15-211c-4039-abf4-0ff37443f3a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c: 1, k: 1\t error: 0.5\n",
            "c: 1, k: 2\t error: 0.31818181818181823\n",
            "c: 1, k: 3\t error: 0.2727272727272727\n",
            "c: 1, k: 4\t error: 0.2272727272727273\n",
            "c: 1, k: 5\t error: 0.2727272727272727\n",
            "c: 1, k: 6\t error: 0.2272727272727273\n",
            "c: 10, k: 1\t error: 0.5\n",
            "c: 10, k: 2\t error: 0.31818181818181823\n",
            "c: 10, k: 3\t error: 0.2727272727272727\n",
            "c: 10, k: 4\t error: 0.2272727272727273\n",
            "c: 10, k: 5\t error: 0.2727272727272727\n",
            "c: 10, k: 6\t error: 0.2272727272727273\n",
            "c: 100, k: 1\t error: 0.5\n",
            "c: 100, k: 2\t error: 0.31818181818181823\n",
            "c: 100, k: 3\t error: 0.2727272727272727\n",
            "c: 100, k: 4\t error: 0.2272727272727273\n",
            "c: 100, k: 5\t error: 0.2727272727272727\n",
            "c: 100, k: 6\t error: 0.2272727272727273\n",
            "c: 1000, k: 1\t error: 0.5\n",
            "c: 1000, k: 2\t error: 0.31818181818181823\n",
            "c: 1000, k: 3\t error: 0.2727272727272727\n",
            "c: 1000, k: 4\t error: 0.2272727272727273\n",
            "c: 1000, k: 5\t error: 0.2727272727272727\n",
            "c: 1000, k: 6\t error: 0.2272727272727273\n",
            "Best c : 1, best k : 4 with acc: 0.7727272727272727\n"
          ]
        }
      ],
      "source": [
        "# Since there are multiple c values that have the highest accuracy\n",
        "# we will arbitrarily pick the lowest c with the highest acc\n",
        "\n",
        "# Split training data into 10 folds\n",
        "best_c = 0\n",
        "best_acc = 0\n",
        "best_k = 0\n",
        "\n",
        "c_values = [1, 10, 100, 1000]\n",
        "k_values = [1, 2, 3, 4, 5, 6]\n",
        "for c in c_values:\n",
        "    for k in k_values:\n",
        "        X_train = decomp.transform(prostate_train[:, :-1], k)\n",
        "        y_train = prostate_train[:, -1]\n",
        "\n",
        "        X_test = decomp.transform(prostate_test[:, :-1], k)\n",
        "        y_test = prostate_test[:, -1]\n",
        "\n",
        "        model = LinearSlackSVM()\n",
        "        model.fit(X_train, y_train, c)\n",
        "        acc = model.accuracy(X_test, y_test)\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_c = c\n",
        "            best_acc = acc\n",
        "            best_k = k\n",
        "        print(f\"c: {c}, k: {k}\\t error: {1 - acc}\")\n",
        "\n",
        "print(f\"Best c : {best_c}, best k : {best_k} with acc: {best_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12TpfJz_NG7C"
      },
      "source": [
        "## 1.2 PCA for Feature Selection (25 pts)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QG-asfMHNLde"
      },
      "source": [
        "1.   Compute the top $k$ eigenvalues and eigenvector of the covariance matrix corresponding to the data matrix omitting the labels (recall that the columns of the data matrix are the input data points). Denote the top $k$ eigenvectors as $v^{(1)}, ... , v^{(k)}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tz8FcQWNPyp"
      },
      "source": [
        "2.   Define $\\pi_j = \\frac{1}{k}\\sum_{i=1}^k v_j^{(i)^2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFN8RbEsNf99"
      },
      "source": [
        "3.   Sample $s$ features independently from the probability distribution defined by $\\pi$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqUwvn8eNlFn"
      },
      "outputs": [],
      "source": [
        "class PCASelection():\n",
        "    def __init__(self):\n",
        "        self.feature_dist = None\n",
        "        self.feature_select = None\n",
        "        self.eigenvalues = None\n",
        "        self.eigenvectors = None\n",
        "    def fit(self, X):\n",
        "        # Step 1: Compupte the eigenvectors\n",
        "        M = np.mean(X, axis=0)\n",
        "        C = X - M\n",
        "        COV = np.matmul(C.T, C)\n",
        "        self.eigenvalues, self.eigenvectors = np.linalg.eig(COV)\n",
        "        self.eigenvalues = np.real(self.eigenvalues)\n",
        "        self.eigenvectors = np.real(self.eigenvectors)\n",
        "        \n",
        "    def select_features(self, k):\n",
        "        # Step 2: Create the probability distribution over the features\n",
        "        self.feature_dist = np.sum(self.eigenvectors[:, :k] ** 2, axis=1) / k\n",
        "        # Renormalize the distribution due to mathmatical imprecisions\n",
        "        self.feature_dist = self.feature_dist / np.sum(self.feature_dist)\n",
        "\n",
        "        # Step 3: sample s features\n",
        "        s = int(k * math.log(k)) if k > 1 else 1\n",
        "        self.feature_select = np.random.choice(range(len(self.feature_dist)), size=s, replace=False, p=self.feature_dist)\n",
        "    def transform(self, X):\n",
        "        return X[:, self.feature_select]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUe4bMP4Nllm"
      },
      "source": [
        "*  Why does $\\pi$ define a probability distribution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Anx-O6d4oYfI"
      },
      "source": [
        "$v^{(i)}_j$ defines the direction of the $i^{th}$ eigenvector with respect to the $j^{th}$ axis. The idea is that this indicates how important $j^{th}$ feature is when computing the linear combination. Because each eigenvectors are normal vectors, the squared sum of each component will add to one. We can therefore add $k$ eigenvectors and normalize it by dividing by $k$ to create a distribution over the original features that estimates how important each feature is when projecting the original data to the $k^{th}$ dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcn5iI_yNsKg"
      },
      "source": [
        "*  Again, using the prostate_GE data set and same procedure as above, for each $k\\in\\{1,10,20,40,80,160\\}$ with $s=\\lfloor k\\log k\\rfloor$, report the average test error of the SVM with slack classifier over $20$ experiments. For each experiment use only the $s$ selected features (note that there may be some duplicates, so only include each feature once). Use the same hyperparameter search for $c$ as in part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4GPGhSnrHPm",
        "outputId": "6d96992d-e81b-41db-8b79-75f5346c4b74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c: 1, k: 1\t average error: 0.41437500000000005\n",
            "c: 1, k: 10\t average error: 0.20125000000000004\n",
            "c: 1, k: 20\t average error: 0.171875\n",
            "c: 1, k: 40\t average error: 0.17500000000000004\n",
            "c: 1, k: 80\t average error: 0.12749999999999995\n",
            "c: 1, k: 160\t average error: 0.114375\n",
            "c: 10, k: 1\t average error: 0.395625\n",
            "c: 10, k: 10\t average error: 0.22999999999999998\n",
            "c: 10, k: 20\t average error: 0.23312500000000003\n",
            "c: 10, k: 40\t average error: 0.16937500000000005\n",
            "c: 10, k: 80\t average error: 0.12187499999999996\n",
            "c: 10, k: 160\t average error: 0.11812500000000004\n",
            "c: 100, k: 1\t average error: 0.37812500000000004\n",
            "c: 100, k: 10\t average error: 0.259375\n",
            "c: 100, k: 20\t average error: 0.218125\n",
            "c: 100, k: 40\t average error: 0.16437500000000005\n",
            "c: 100, k: 80\t average error: 0.12437500000000001\n",
            "c: 100, k: 160\t average error: 0.12312500000000004\n",
            "c: 1000, k: 1\t average error: 0.39875000000000005\n",
            "c: 1000, k: 10\t average error: 0.266875\n",
            "c: 1000, k: 20\t average error: 0.23250000000000004\n",
            "c: 1000, k: 40\t average error: 0.16562500000000002\n",
            "c: 1000, k: 80\t average error: 0.13\n",
            "c: 1000, k: 160\t average error: 0.10624999999999996\n",
            "Best c : 1000, best k : 160 with error: 0.10624999999999996\n",
            "Testing accuracy: 0.8636363636363636\n"
          ]
        }
      ],
      "source": [
        "# Since there are multiple c values that have the highest accuracy\n",
        "# we will arbitrarily pick the lowest c with the highest acc\n",
        "\n",
        "decomp = PCASelection()\n",
        "decomp.fit(prostate_train[:,:-1])\n",
        "\n",
        "# Split training data into 10 folds\n",
        "folds = np.split(prostate_train, 10)\n",
        "best_c = 0\n",
        "best_acc = 0\n",
        "best_k = 0\n",
        "\n",
        "c_values = [1, 10, 100, 1000]\n",
        "k_values = [1, 10, 20, 40, 80, 160]\n",
        "for c in c_values:\n",
        "    for k in k_values:\n",
        "        avg_acc = 0\n",
        "        # Repeat each hyperparameter pair over 20 CV\n",
        "        for _ in range(20):\n",
        "            for i in range(len(folds)):\n",
        "                data = [x for j, x in enumerate(folds) if j != i] \n",
        "                data = np.concatenate(data)\n",
        "\n",
        "                decomp.select_features(k)\n",
        "\n",
        "                X_train = decomp.transform(data[:, :-1])\n",
        "                y_train = data[:, -1]\n",
        "                X_val = decomp.transform(folds[i][:, :-1])\n",
        "                y_val = folds[i][:, -1]\n",
        "\n",
        "                model = LinearSlackSVM()\n",
        "                model.fit(X_train, y_train, c)\n",
        "                val_acc = model.accuracy(X_val, y_val)\n",
        "                avg_acc += val_acc\n",
        "        avg_acc /= len(folds) * 20\n",
        "        if avg_acc > best_acc:\n",
        "            best_c = c\n",
        "            best_acc = avg_acc\n",
        "            best_k = k\n",
        "        print(f\"c: {c}, k: {k}\\t average error: {1 - avg_acc}\")\n",
        "\n",
        "print(f\"Best c : {best_c}, best k : {best_k} with error: {1 - best_acc}\")\n",
        "\n",
        "decomp.select_features(best_k)\n",
        "\n",
        "# Retrain on all training data\n",
        "X_train = decomp.transform(prostate_train[:, :-1])\n",
        "y_train = prostate_train[:, -1]\n",
        "\n",
        "model = LinearSlackSVM()\n",
        "model.fit(X_train, y_train, best_c)\n",
        "\n",
        "# Test on testing set\n",
        "X_test = decomp.transform(prostate_test[:, :-1])\n",
        "y_test = prostate_test[:, -1]\n",
        "test_acc = model.accuracy(X_test, y_test)\n",
        "\n",
        "print(f\"Testing accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKPQaKzNOMHJ"
      },
      "source": [
        "* Does this provide a reasonable alternative to the SVM with slack formulation without feature\n",
        "selection on this data set? What are the pros and cons of this approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPhQH2Do1dO4"
      },
      "source": [
        "The feature selection does provide a resonable tradeoff between accuracy and computational performance. The pro of this approach is that it drastically reduces the power needed to both train and predict using this model. This may be beneficial in low power situation or when collecting every feature is costly. The con is that the model sacrifices some accuracy. Additionally if the collected data is not representative of real world data, the feature selection may discard features that are able to generalize better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P__DNAuRONHr",
        "outputId": "14075f0b-5b93-4d92-94fe-433c0b400cb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing accuracy: 0.9090909090909091\n"
          ]
        }
      ],
      "source": [
        "# Retrain on all training data\n",
        "X_train = prostate_train[:, :-1]\n",
        "y_train = prostate_train[:, -1]\n",
        "\n",
        "model = LinearSlackSVM()\n",
        "model.fit(X_train, y_train, best_c)\n",
        "\n",
        "# Test on testing set\n",
        "X_test = prostate_test[:, :-1]\n",
        "y_test = prostate_test[:, -1]\n",
        "test_acc = model.accuracy(X_test, y_test)\n",
        "\n",
        "print(f\"Testing accuracy: {test_acc}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3JMJTbi6OTGW"
      },
      "source": [
        "# Problem 2: Working with k-means"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ftEKxSWHObwB"
      },
      "source": [
        "For this problem, you will use the leaf.data file provided with this problem set. This data set\n",
        "was generated from the UCI Leaf Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U98KPUrE3dhW"
      },
      "outputs": [],
      "source": [
        "# Preprocess data\n",
        "X = leaf_data[:, 1:]\n",
        "M = np.mean(X, axis = 0)\n",
        "sd = np.std(X, axis = 0)\n",
        "X = ((X - M) / sd)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HcKKLATuOdlL"
      },
      "source": [
        "## 2.1 Train a k-means classifier for each $k\\in\\{10, 15, 20, 25, 30\\}$ starting from twenty different random initializations (sample uniformly from $[−3, 3]$ for each attribute) for each $k$. Report the mean and variance of the value of the $k$-means objective obtained for each $k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebNkxePLOX2C",
        "outputId": "f12eb6eb-73b9-4d1d-82c0-4f9c64c824e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k = 10, mean = 1354.7801886138723, var = 74647.95673443752\n",
            "k = 15, mean = 1110.8436986486881, var = 25612.508374235716\n",
            "k = 20, mean = 973.3956873904133, var = 36144.8604780891\n",
            "k = 25, mean = 913.9443494669492, var = 23909.280633463153\n",
            "k = 30, mean = 850.3265327948357, var = 12049.31990407491\n"
          ]
        }
      ],
      "source": [
        "class KMeans():\n",
        "    def __init__(self):\n",
        "        self.centers = None\n",
        "    def fit(self, X, k):\n",
        "        # Initalize means\n",
        "        new_centers = np.random.random_sample((k, X.shape[1])) * 6 - 3\n",
        "        self.centers = new_centers + 1\n",
        "\n",
        "        while (np.sum(self.centers - new_centers) != 0):\n",
        "            self.centers = new_centers\n",
        "            # Nested list where each list represent all points share the same\n",
        "            # ith mean\n",
        "            groups = [[] for _ in range(k)]\n",
        "            # loop through all points\n",
        "            for x in X:\n",
        "                # Calculate distances from current point to means\n",
        "                distances = [np.linalg.norm(x - center) for center in self.centers]\n",
        "                # Add point to the corresponding group\n",
        "                group = np.argmin(distances)\n",
        "                groups[group].append(x)\n",
        "\n",
        "            # Calculate new means, if no points are in a group, use prev mean\n",
        "            new_centers = np.array([np.mean(np.array(group), axis=0) if len(group) != 0 else center for group, center in zip(groups, self.centers)])\n",
        "\n",
        "        # Calculate objective function\n",
        "        objective = 0\n",
        "        for group, center in zip(groups, self.centers):\n",
        "            for point in group:\n",
        "                objective += np.linalg.norm(point - center) ** 2\n",
        "            \n",
        "        return objective\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "k_values = [10, 15, 20, 25, 30]\n",
        "\n",
        "for k in k_values:\n",
        "    model = KMeans()\n",
        "    runs = [model.fit(X, k) for _ in range(20)]\n",
        "    print(f\"k = {k}, mean = {np.mean(runs)}, var = {np.var(runs)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_UHDwrYMOz-D"
      },
      "source": [
        "## 2.2 Random initializations can easily get stuck in suboptimal clusterings. An improvement of the k-means algorithm, known as k-means++, instead chooses an initialization as follows:\n",
        "\n",
        "  (a) Choose a data point uniformly at random to be the first center."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3pYJV1sO-lY"
      },
      "source": [
        "(b) Repeat the following until k centers have been selected:  \n",
        "i. For each data point x compute the distance between x and the nearest cluster center\n",
        "in the current set of centers. Denote this distance as dx."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYGInQKqPGP5"
      },
      "source": [
        "ii. Sample a training data point at random from the distribution $p$ such that $p(x) ∝ d^2_x $\n",
        "Add the sampled point to the current set of centers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcpuTz4KPhqH"
      },
      "source": [
        "Repeat the first experiment using this initialization to pick the initial cluster centers for\n",
        "k-means. Does this procedure result in an improvement? Explain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8Uzbj01PMMz",
        "outputId": "47d66ecf-5913-4322-bdf4-a74bc5787d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k = 10, mean = 1046.667471019411, var = 3209.71939637129\n",
            "k = 15, mean = 758.1106448760277, var = 1127.8378654930525\n",
            "k = 20, mean = 608.3284704794128, var = 453.2491002451964\n",
            "k = 25, mean = 526.8292057692009, var = 403.7043940791069\n",
            "k = 30, mean = 452.3543559247426, var = 279.7483333033425\n"
          ]
        }
      ],
      "source": [
        "class KMeansplus():\n",
        "    def __init__(self):\n",
        "        self.centers = None\n",
        "    def fit(self, X, k):\n",
        "        \n",
        "        # Kmeans++ initlization ----------------\n",
        "\n",
        "        # list of points that are not means\n",
        "        points = [row for row in X]\n",
        "        # list of points that are means\n",
        "        centers = [points.pop(random.randint(0, len(points)))]\n",
        "\n",
        "        while (len(centers) != k):\n",
        "            min_distances_squared = []\n",
        "            for point in points:\n",
        "                # calculate the min distance between current point with all centers\n",
        "                min_distance = min([np.linalg.norm(point - center) for center in centers])\n",
        "                min_distances_squared.append(min_distance ** 2)\n",
        "\n",
        "            # Scale the distances squared to create a probability distribution\n",
        "            min_distances_squared = [dist / sum(min_distances_squared) for dist in min_distances_squared]\n",
        "\n",
        "            # Sample dist to get next center\n",
        "            next_center = np.random.choice(range(len(min_distances_squared)), p=min_distances_squared)\n",
        "            centers.append(points.pop(next_center))\n",
        "\n",
        "        new_centers = np.array(centers)\n",
        "        self.centers = new_centers + 1\n",
        "        # ------------------------\n",
        "\n",
        "        while (np.sum(self.centers - new_centers) != 0):\n",
        "            #print(f\"dif = {np.sum(self.centers - new_centers)}\")\n",
        "            self.centers = new_centers\n",
        "            groups = [[] for _ in range(k)]\n",
        "            for x in X:\n",
        "                distances = [np.linalg.norm(x - center) for center in self.centers]\n",
        "                group = np.argmin(distances)\n",
        "                groups[group].append(x)\n",
        "\n",
        "            # Calculate new means, if no points are in a group, use prev mean\n",
        "            new_centers = np.array([np.mean(np.array(group), axis=0) if len(group) != 0 else center for group, center in zip(groups, self.centers)])\n",
        "\n",
        "        # Calculate objective function\n",
        "        objective = 0\n",
        "        for group, center in zip(groups, self.centers):\n",
        "            for point in group:\n",
        "                objective += np.linalg.norm(point - center) ** 2\n",
        "            \n",
        "        return objective\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "k_values = [10, 15, 20, 25, 30]\n",
        "\n",
        "for k in k_values:\n",
        "    model = KMeansplus()\n",
        "    runs = [model.fit(X, k) for _ in range(20)]\n",
        "    print(f\"k = {k}, mean = {np.mean(runs)}, var = {np.var(runs)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ElsWo254__o"
      },
      "source": [
        "It seems like kmeans++ did yield an improvement over the original implementation. The mean and variance of the objective is smaller implying that points are more consistently closer to the means. By selecting points through a probability distribution of the min distances, the initalization is more evenly spread out with respect to the data. In constrast, a random uniform initalization, may lead to some center means beinging grouped together while others are far away. This causes some centers to group a majority of the data points while the far away centers will have little to no data points in its group."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:51:29) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "eee0ec1dfc014c5e818c9fd6187bf3227a654b7ffe32fcac85496f478f59351d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
