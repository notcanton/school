{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Environment:\n",
    "python=3.8\n",
    "numpy\n",
    "cvxopt\n",
    "sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery_path = './mystery.data'\n",
    "perceptron_path = './perceptron.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import cvxopt\n",
    "# cvxopt.solvers.options['maxiters'] = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(num):\n",
    "    if num > 0:\n",
    "        return 1\n",
    "    elif num < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perception Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the perceptron algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_data = np.genfromtxt(perceptron_path, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    def __init__(self):\n",
    "        self.w = np.zeros(4)\n",
    "        self.b = 0\n",
    "    def predict(self, x):\n",
    "        return self.w @ x + self.b \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the number of misclassified datapoints\n",
    "\n",
    "def misclassify(model, data):\n",
    "    miss = 0\n",
    "    for row in data:\n",
    "        x = row[:-1]\n",
    "        y_predict = model.predict(x)\n",
    "        y = row[-1]\n",
    "        if (sign(y_predict) != y):\n",
    "            miss += 1\n",
    "    return miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and returns the gradient of w and b\n",
    "\n",
    "def grad(model, data):\n",
    "\n",
    "    M = data.shape[0]\n",
    "    w_grad = 0\n",
    "    b_grad = 0\n",
    "    \n",
    "    for point in data:\n",
    "        x = point[:-1]\n",
    "        y_predict = model.predict(x)\n",
    "        y = point[-1]\n",
    "\n",
    "        # if point is misclassified\n",
    "        if ((-y * y_predict) >= -1):\n",
    "            # update gradient\n",
    "            w_grad += 2 * x * y * (1 - (y_predict * y))\n",
    "            b_grad += 2 * y * (1 - (y_predict * y))\n",
    "    \n",
    "    return (w_grad / M, b_grad / M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycles through the data points in the data sets\n",
    "# Used for stochastic gradient descent\n",
    "\n",
    "def cyclenext(data):\n",
    "    while True:\n",
    "        for point in data:\n",
    "            yield np.reshape(point, (1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the loss of a model\n",
    "\n",
    "def loss(model, data):\n",
    "    loss = 0\n",
    "    M = data.shape[0]\n",
    "    for point in data:\n",
    "        x = point[:-1]\n",
    "        y_predict = model.predict(x)\n",
    "        y = point[-1]\n",
    "        loss += max(0, 1 - y * y_predict)**2\n",
    "    return loss / M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs stochastic gradient descent\n",
    "# Returns the number of iterations it took\n",
    "# Prints the parameters for the first three iterations\n",
    "\n",
    "def stoch_train(model, data, step_size):\n",
    "    itera = 0\n",
    "    miss = misclassify(model, data)\n",
    "    point = cyclenext(data)\n",
    "\n",
    "    # Continue until there are 0 misclassified data points\n",
    "    while miss > 0:       \n",
    "        if itera <= 3:\n",
    "            print(\"Iteration: \"+ str(itera))\n",
    "            print(\"W:\" + str(model.w))\n",
    "            print(\"b:\" + str(model.b))\n",
    "        else:\n",
    "            print(\"Iter: \" + str(itera) + \"\\tMiss: \" + str(miss) + \"\\tLoss: \" + str(los), end='\\r', flush=True)\n",
    "\n",
    "        w_grad, b_grad = grad(model, next(point))\n",
    "        los = loss(model, data)\n",
    "\n",
    "        # If the gradient explodes exit\n",
    "        if (los == float('inf')):\n",
    "            print(\"exploded\")\n",
    "            return itera\n",
    "\n",
    "        # update parameters\n",
    "        model.w = model.w  + (step_size * w_grad)\n",
    "        model.b = model.b  + (step_size * b_grad)\n",
    "        # plt.plot(itera, los, marker='.', color=\"blue\")\n",
    "        itera += 1\n",
    "\n",
    "        miss = misclassify(model, data)\n",
    "        \n",
    "    return itera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs batch gradient descent\n",
    "# Returns the number of iterations it took\n",
    "# Prints the parameters for the first three iterations\n",
    "\n",
    "def batch_train(model, data, step_size):\n",
    "    itera = 0\n",
    "    miss = misclassify(model, data)\n",
    "\n",
    "    # Continue until there are 0 misclassified data points\n",
    "    while miss > 0:       \n",
    "        if itera <= 3:\n",
    "            print(\"Iteration: \"+ str(itera))\n",
    "            print(\"W:\" + str(model.w))\n",
    "            print(\"b:\" + str(model.b))\n",
    "        else:\n",
    "            print(\"Iter: \" + str(itera) + \"\\tMiss: \" + str(miss) + \"\\tLoss: \" + str(los), end='\\r', flush=True)\n",
    "\n",
    "        w_grad, b_grad = grad(model, data)\n",
    "        los = loss(model, data)\n",
    "\n",
    "        # If the gradient explodes exit\n",
    "        if (los == float('inf')):\n",
    "            print(\"exploded\")\n",
    "            return itera\n",
    "\n",
    "        # update parameters\n",
    "        model.w = model.w  + (step_size * w_grad)\n",
    "        model.b = model.b  + (step_size * b_grad)\n",
    "        # plt.plot(itera, los, marker='.', color=\"blue\")\n",
    "        itera += 1\n",
    "\n",
    "        miss = misclassify(model, data)\n",
    "\n",
    "    return itera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Train a perceptron model. Initalize a perceptron object. Pass it through either batch_train or stoch_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "W:[0. 0. 0. 0.]\n",
      "b:0\n",
      "Iteration: 1\n",
      "W:[ 1.14605485  0.41072288 -0.0970209  -1.49855944]\n",
      "b:-0.3182562562562562\n",
      "Iteration: 2\n",
      "W:[ 1.2517209   0.36676973  0.13570134 -1.11579275]\n",
      "b:-0.6224974503909013\n",
      "Iteration: 3\n",
      "W:[ 0.7511016   0.3339475  -0.06249071 -1.34243463]\n",
      "b:-0.879442595539019\n",
      "Iter: 426\tMiss: 1\tLoss: 0.0021507401698985474\n",
      "Final\n",
      "W: [ 330.14092297  115.66126685    3.11450368 -383.10862723]\n",
      "b: -706.0978002173999\n"
     ]
    }
   ],
   "source": [
    "# Batch training\n",
    "model = perceptron()\n",
    "batch_train(model, perceptron_data, 0.4478)\n",
    "\n",
    "# Stochastic training\n",
    "# model = perceptron()\n",
    "# stoch_train(model, perceptron_data, 0.01)\n",
    "\n",
    "print(\"\\nFinal\")\n",
    "print(\"W: \" + str(model.w))\n",
    "print(\"b: \" + str(model.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative to sklearn.preprocessing.PolynomialFeatures\n",
    "\n",
    "def featurize_vector(X, degree) :\n",
    "    features = X.copy()\n",
    "    prev_chunk = X\n",
    "    indices = list(range(len(X)))\n",
    "    features = np.insert(features, 0, 1)\n",
    "    for _ in range(1, degree):\n",
    "        new_chunk = []\n",
    "        for i, v in enumerate(X) :\n",
    "            next_index = len(new_chunk)\n",
    "            for coef in prev_chunk[indices[i]:] :\n",
    "                new_chunk.append(v * coef)\n",
    "            indices[i] = next_index\n",
    "        features = np.append(features, new_chunk)\n",
    "        prev_chunk = new_chunk\n",
    "\n",
    "    return features\n",
    "\n",
    "def polynomialFeatures(X, degree):\n",
    "    result = []\n",
    "    for row in X:\n",
    "        result.append(featurize_vector(row, degree))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data and apply any featurization\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Featurize the data to a polynomial of degree 3\n",
    "poly = preprocessing.PolynomialFeatures(degree=3)\n",
    "mystery_data = np.genfromtxt(mystery_path, delimiter=',')\n",
    "\n",
    "X = mystery_data[:,:-1]\n",
    "y = mystery_data[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         -1.34616021 -2.10239244 -0.55455071  0.30173198  1.8121473\n",
      "  2.83015704  0.7465141  -0.40617959  4.42005398  1.16588322 -0.63435904\n",
      "  0.30752649 -0.16732569  0.09104219 -2.43944059 -3.80984479 -1.00492757\n",
      "  0.5467828  -5.95010078 -1.5694656   0.8539489  -0.41397992  0.22524718\n",
      " -0.12255737 -9.29268808 -2.45114407  1.33367165 -0.64654137  0.35178426\n",
      " -0.19140641 -0.17053903  0.09279058 -0.05048751  0.02747034]\n",
      "[ 1.         -1.34616021 -2.10239244 -0.55455071  0.30173198  1.8121473\n",
      "  2.83015704  0.7465141  -0.40617959  4.42005398  1.16588322 -0.63435904\n",
      "  0.30752649 -0.16732569  0.09104219 -2.43944059 -3.80984479 -1.00492757\n",
      "  0.5467828  -5.95010078 -1.5694656   0.8539489  -0.41397992  0.22524718\n",
      " -0.12255737 -9.29268808 -2.45114407  1.33367165 -0.64654137  0.35178426\n",
      " -0.19140641 -0.17053903  0.09279058 -0.05048751  0.02747034]\n"
     ]
    }
   ],
   "source": [
    "# Validate that sklearn polynomial features produces the same result as polynomialFeatures()\n",
    "\n",
    "X1 = polynomialFeatures(X,3)\n",
    "X2 = poly.fit_transform(X)\n",
    "\n",
    "print(X1[-1])\n",
    "print(X2[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the SVM\n",
    "\n",
    "Perform training to produce $W$ and $b$ for the svm\n",
    "\n",
    "$$\\min_{w,b} = \\frac{1}{||w||}$$\n",
    "\n",
    "$$s.t. \\space y^{(i)}(w^Tx^{(i)}+b) \\geq 1$$\n",
    "\n",
    "The primal problem is converted into the standard form for CVOPT such that\n",
    "$$ x = [w, b]$$\n",
    "\n",
    "For full equation see the write up.\n",
    "The training function would return the optimal $W$ and $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X, y):\n",
    "    num_data = X.shape[0]\n",
    "    dim_data = X.shape[1] + 1\n",
    "    P = np.identity(dim_data)\n",
    "    P[-1,-1] = 0\n",
    "    P = cvxopt.matrix(P)\n",
    "\n",
    "    q = cvxopt.matrix(np.zeros(dim_data))\n",
    "\n",
    "    G = np.zeros((num_data, dim_data))\n",
    "\n",
    "\n",
    "    for i, row in enumerate(X):\n",
    "        G[i] = np.append(-y[i] * row[:], -y[i])\n",
    "\n",
    "    G = cvxopt.matrix(G)\n",
    "\n",
    "    h = cvxopt.matrix(-np.ones(num_data))\n",
    "\n",
    "    solution = cvxopt.solvers.qp(P, q, G, h)\n",
    "    w = np.array(solution['x']).flatten()[:-1]\n",
    "    b = np.array(solution['x']).flatten()[-1]\n",
    "    return (w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.4983e-02  1.7674e+03  6e+03  3e+00  5e+04\n",
      " 1:  7.0139e-03  8.5296e+02  4e+03  2e+00  3e+04\n",
      " 2:  1.4334e-02  1.1209e+03  4e+03  1e+00  2e+04\n",
      " 3:  1.6587e-02  2.9810e+03  4e+03  1e+00  2e+04\n",
      " 4:  2.9165e-02  9.2979e+03  5e+03  1e+00  2e+04\n",
      " 5:  4.1615e-02  1.6123e+04  6e+03  1e+00  2e+04\n",
      " 6:  1.0022e-01  3.3099e+04  8e+03  1e+00  2e+04\n",
      " 7:  3.5155e-01  3.6599e+04  1e+04  1e+00  2e+04\n",
      " 8:  9.4397e-01  2.6114e+04  2e+04  1e+00  2e+04\n",
      " 9:  2.5906e+00  1.0275e+04  3e+04  9e-01  2e+04\n",
      "10:  7.9309e+00 -1.5261e+04  3e+04  7e-01  1e+04\n",
      "11:  1.8393e+01 -1.7767e+04  2e+04  4e-01  7e+03\n",
      "12:  3.0244e+01 -3.5403e+03  4e+03  4e-02  8e+02\n",
      "13:  3.1207e+01 -6.5082e+01  1e+02  8e-04  2e+01\n",
      "14:  2.3931e+01  8.4801e+00  2e+01  9e-05  2e+00\n",
      "15:  2.2166e+01  1.6164e+01  6e+00  2e-05  3e-01\n",
      "16:  2.1854e+01  1.8311e+01  4e+00  9e-06  2e-01\n",
      "17:  2.1652e+01  1.9320e+01  2e+00  3e-06  7e-02\n",
      "18:  2.1486e+01  2.1099e+01  4e-01  6e-07  1e-02\n",
      "19:  2.1455e+01  2.1444e+01  1e-02  2e-08  3e-04\n",
      "20:  2.1455e+01  2.1454e+01  1e-04  2e-10  3e-06\n",
      "21:  2.1455e+01  2.1455e+01  1e-06  2e-12  3e-08\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "w, b = train_svm(X1,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the optimal Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM vector would be in the form:\n",
    "$$w^T \\phi (x) + b = 0$$\n",
    "\n",
    "and the support vectors would be in the form: \n",
    "$$w^T \\phi (x) + b = \\pm 1$$\n",
    "\n",
    "Where $\\phi (x)$ is the featurized x vector. In this case a polynomial of degree 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [-2.71771090e-15  2.72684628e-01 -4.12665759e-01  1.52561747e-01\n",
      " -6.65337251e-01 -4.40721095e-02  1.45675873e-01  1.01835956e-01\n",
      "  2.36239852e-01  6.89736689e-02  1.54089640e-01  4.34935391e-04\n",
      "  1.15905787e-01  2.07438344e-02  2.82208787e-02  7.89709193e-03\n",
      " -5.00620758e-02 -2.60479074e-02  2.94319257e+00  1.19497072e-02\n",
      "  5.77694673e+00 -1.20568479e-02 -1.24233969e-02 -7.97262404e-02\n",
      "  1.80846152e-02  8.55298187e-02 -3.24304981e-02 -3.01570959e-02\n",
      "  4.72229196e-02  8.11004480e-02 -3.21421266e-02 -4.89785355e-04\n",
      " -1.55462570e-03 -3.09482104e-02 -1.32164120e-03]\n",
      "b: -1.317452061696287\n"
     ]
    }
   ],
   "source": [
    "print(\"w: \" + str(w))\n",
    "print (\"b: \" + str(b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caluclating the Margin\n",
    "\n",
    "We can find the margin of the SVM by using:\n",
    "$$ \\frac{1}{||w||} $$\n",
    "\n",
    "Which would the maximum distance of the closest point to the hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023305105615744854\n"
     ]
    }
   ],
   "source": [
    "margin = 1/(w @ w)\n",
    "print(margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest point: [ 1.00000000e+00 -1.09606675e-01 -3.96745057e-01  2.60957470e+00\n",
      " -1.03447153e+00  1.20136232e-02  4.34859065e-02 -2.86026806e-01\n",
      "  1.13384984e-01  1.57406640e-01 -1.03533586e+00  4.10421464e-01\n",
      "  6.80988012e+00 -2.69953072e+00  1.07013134e+00 -1.31677329e-03\n",
      " -4.76634562e-03  3.13504472e-02 -1.24277511e-02 -1.72528185e-02\n",
      "  1.13479722e-01 -4.49849320e-02 -7.46408317e-01  2.95886586e-01\n",
      " -1.17293538e-01 -6.24503065e-02  4.10764386e-01 -1.62832687e-01\n",
      " -2.70178628e+00  1.07102547e+00 -4.24569318e-01  1.77708909e+01\n",
      " -7.04462707e+00  2.79258766e+00 -1.10702040e+00] with label: 1.0 and is off by the theoretical: 1.020347806246491e-09\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any points that are still misclassified\n",
    "# Prints the closet point to the hyperplace (i.e. the point on the support vectors)\n",
    "\n",
    "off = 1\n",
    "for point, label in zip(X, y):\n",
    "    y_pred = w @ point + b\n",
    "\n",
    "    if (abs(y_pred) - 1 < off):\n",
    "        off = abs(y_pred) - 1\n",
    "        x_closest = point\n",
    "        y_closest = label\n",
    "\n",
    "\n",
    "    if sign(y_pred) != label:\n",
    "        print(f\"Worng: {label} {y_pred}\")\n",
    "print(f\"Closest point: {x_closest} with label: {y_closest} and is off by the theoretical: {off}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "360a3679dc950de283638f2633770a78c2574093e8e93eb4b8a17186cb255508"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('personal': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
