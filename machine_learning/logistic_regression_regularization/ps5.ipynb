{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2IfCluMDNkNf"
      },
      "source": [
        "# Problem Set 5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JZm3OmSgPmIk"
      },
      "source": [
        "# Problem 2: Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsjrXoudNCs-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "park_train_path = \"./park_train.data\"\n",
        "park_validation_path = \"./park_validation.data\"\n",
        "park_test_path = \"./park_test.data\"\n",
        "\n",
        "park_train = np.genfromtxt(park_train_path, delimiter=',')\n",
        "park_val = np.genfromtxt(park_validation_path, delimiter=',')\n",
        "park_test = np.genfromtxt(park_test_path, delimiter=\",\")\n",
        "\n",
        "X_train = park_train[:,1:]\n",
        "y_train = park_train[:,0]\n",
        "y_train[y_train == 0] = -1\n",
        "\n",
        "X_val = park_val[:,1:]\n",
        "y_val = park_val[:,0]\n",
        "y_val[y_val == 0] = -1\n",
        "\n",
        "X_test = park_test[:,1:]\n",
        "y_test = park_test[:,0]\n",
        "y_test[y_test == 0] = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at38OVNsNCs_"
      },
      "outputs": [],
      "source": [
        "# tolerance: controls the stopping condition. When the max graident < tolerance,\n",
        "#            training stops\n",
        "# max_iter: controls the max iteration before stopping\n",
        "\n",
        "class LogisticRegression():\n",
        "    def __init__(self, X):\n",
        "        self.w = np.ones(X.shape[1]) * 0.5\n",
        "        self.b = 0.5\n",
        "        self.tolerance = 1e-4\n",
        "        self.max_iter = 1000000\n",
        "\n",
        "    def prob_one(self, X):\n",
        "        lin = np.dot(X, self.w) + self.b\n",
        "\n",
        "        # exp = np.exp(lin)\n",
        "        # return exp / (1 + exp)\n",
        "\n",
        "        # Alternative form to compute the probability that is more stable\n",
        "        # and less prone to overflowing:\n",
        "        # https://shusei-e.github.io/machine%20learning/sigmoid_tanh/\n",
        "        return 1 - (np.tanh(lin * -0.5) * 0.5 + 0.5)\n",
        "        \n",
        "    def fit(self, X, y, step_size, lamb=0, reg=None):\n",
        "        y_term = (y + 1) / 2\n",
        "        i = 0\n",
        "\n",
        "        while (True):\n",
        "            prob = self.prob_one(X)\n",
        "            inner_term = y_term - prob\n",
        "\n",
        "            # Regularization terms\n",
        "            if reg == \"l1\":\n",
        "                reg_term = np.ones(X.shape[1])\n",
        "                reg_term[self.w < 0] = -1\n",
        "            elif reg == \"l2\":\n",
        "                reg_term = self.w\n",
        "            else:\n",
        "                reg_term = 0\n",
        "\n",
        "            # Gradients\n",
        "            b_grad = inner_term.sum()\n",
        "            w_grad =  np.dot(X.T, inner_term) - (lamb * reg_term)             \n",
        "\n",
        "            # Stopping condition\n",
        "            if (b_grad < self.tolerance and (w_grad < self.tolerance).sum() == X.shape[1]):\n",
        "                break\n",
        "\n",
        "            # Detects if an overflow occured and terminates\n",
        "            # Occurs when step size is too big\n",
        "            if (np.isnan(b_grad) or np.isnan(w_grad).sum() > 0):\n",
        "                print(\"Explode\")\n",
        "                break\n",
        "\n",
        "            # Update Parameters\n",
        "            self.b = self.b + step_size * b_grad\n",
        "            self.w = self.w + step_size * w_grad\n",
        "\n",
        "            # if(i % 1000 == 0):\n",
        "            #     print(f\"iter {i}: b = {b_grad}\\tw = {np.mean(w_grad)}\")\n",
        "            i += 1\n",
        "            \n",
        "            if (i > self.max_iter):\n",
        "                break\n",
        "        return i\n",
        "\n",
        "    def predict(self, X):\n",
        "        prob = self.prob_one(X)\n",
        "        #print(prob)\n",
        "        ans = np.zeros(X.shape[0])\n",
        "        ans[prob > 0.5] = 1\n",
        "        ans[prob <= 0.5] = -1\n",
        "        return ans\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        #print(y_pred)\n",
        "        correct = np.equal(y, y_pred)\n",
        "        return np.count_nonzero(correct) / len(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxdG4GO-NCtA",
        "outputId": "83a152f9-7f24-4798-fd3b-f9e8a1ab71dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No regularization Test accuracy = 0.864406779661017\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression(X_train)\n",
        "model.fit(X_train, y_train, 1e-6)\n",
        "acc = model.score(X_test, y_test)\n",
        "print(f\"No regularization Test accuracy = {acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whq1e_8iNCtB"
      },
      "source": [
        "For linearly separable data, the best separator would be a sharp decision boundary such that we have 100% probability on either side of the boundary. For a 2-D logistic regression, the decision boundary would be in the form:\n",
        "\n",
        "$$\\frac {1}{1 + e^{wx+b}}$$\n",
        "\n",
        "For a logistic curve to have a sharp decision boundary, we would want to maximize the denominator such that $P(Y=-1|x)$ would approach 0. Therefore, $w$ would have to approach infinity. In gradient descent, $w$ would increase without bound and never converge."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w92mU9UTSbvz"
      },
      "source": [
        "## 2. Fit a logistic regression classifier with an $l2$ penalty on the weights to this data set using the validation set to select a good choice of the regularization constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BEw43YVNCtB",
        "outputId": "78eb9f8f-df2f-46ee-978e-c5ffef6f0955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lambda = 10000           Val Acc = 0.7413793103448276\n",
            "lambda = 1000            Val Acc = 0.7413793103448276\n",
            "lambda = 100             Val Acc = 0.7241379310344828\n",
            "lambda = 10              Val Acc = 0.7931034482758621\n",
            "lambda = 1               Val Acc = 0.8275862068965517\n",
            "lambda = 0.1             Val Acc = 0.8275862068965517\n",
            "lambda = 0.01            Val Acc = 0.8275862068965517\n",
            "lambda = 0.001           Val Acc = 0.8275862068965517\n",
            "lambda = 0.0001          Val Acc = 0.8275862068965517\n",
            "Best lambda = 1\n",
            "Test acc = 0.847457627118644\n",
            "w = [ 0.01215456 -0.00683945 -0.01141767  0.18980165  0.18398946  0.18895475\n",
            "  0.18846673  0.19898256  0.25724945  0.89295682  0.22432296  0.22799714\n",
            "  0.23893422  0.30509848  0.19234986  0.06944765  0.47427213  0.62068363\n",
            "  0.95185286  0.49017007  1.69842351  0.53127027]\n",
            "b = 1.2182339551145018\n"
          ]
        }
      ],
      "source": [
        "lambda_values = [10**-i for i in range(-4,5)]\n",
        "results = []\n",
        "\n",
        "for lamb in lambda_values:\n",
        "    model = LogisticRegression(X_train)\n",
        "    model.fit(X_train, y_train, 1e-6, lamb, \"l2\")\n",
        "    acc = model.score(X_val, y_val)\n",
        "    print(f\"lambda = {lamb :<15} Val Acc = {acc}\")\n",
        "    results.append((lamb, model, acc))\n",
        "\n",
        "best_result = max(results, key= lambda x : x[2])\n",
        "print(f\"Best lambda = {best_result[0]}\")\n",
        "print(f\"Test acc = {best_result[1].score(X_test, y_test)}\")\n",
        "\n",
        "print(f\"w = {best_result[1].w}\")\n",
        "print(f\"b = {best_result[1].b}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oDtEoDOpShq3"
      },
      "source": [
        "## 3. Fit a logistic regression classifier with an $l1$ penalty on the weights to this data set using the validation set to select a good choice of the regularization constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmE3zOCuNCtB",
        "outputId": "30201e77-e9ff-459e-cf52-669d77a62d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lambda = 10000           Val Acc = 0.7413793103448276\n",
            "lambda = 1000            Val Acc = 0.7413793103448276\n",
            "lambda = 100             Val Acc = 0.7241379310344828\n",
            "lambda = 10              Val Acc = 0.7241379310344828\n",
            "lambda = 1               Val Acc = 0.8448275862068966\n",
            "lambda = 0.1             Val Acc = 0.8275862068965517\n",
            "lambda = 0.01            Val Acc = 0.8275862068965517\n",
            "lambda = 0.001           Val Acc = 0.8275862068965517\n",
            "lambda = 0.0001          Val Acc = 0.8275862068965517\n",
            "Best lambda = 1\n",
            "Test acc = 0.847457627118644\n",
            "w = [ 8.58443367e-03 -6.86155805e-03 -1.03163744e-02  6.19464307e-07\n",
            " -6.58946167e-07 -2.97064427e-07 -5.83767405e-07 -3.20826751e-07\n",
            " -7.10010016e-08  7.31718319e-01  9.31592925e-07 -3.51547379e-07\n",
            " -4.40646323e-07  6.68815460e-07 -2.26637843e-07  4.58820597e-02\n",
            "  5.19397726e-02  2.79585618e-01  8.64917500e-01  4.31893089e-02\n",
            "  2.16634026e+00  1.26268080e-01]\n",
            "b = 1.2797651190739914\n"
          ]
        }
      ],
      "source": [
        "lambda_values = [10**-i for i in range(-4,5)]\n",
        "results = []\n",
        "\n",
        "for lamb in lambda_values:\n",
        "    model = LogisticRegression(X_train)\n",
        "    model.fit(X_train, y_train, 1e-6, lamb, \"l1\")\n",
        "    acc = model.score(X_val, y_val)\n",
        "    print(f\"lambda = {lamb :<15} Val Acc = {acc}\")\n",
        "    results.append((lamb, model, acc))\n",
        "\n",
        "best_result = max(results, key= lambda x : x[2])\n",
        "print(f\"Best lambda = {best_result[0]}\")\n",
        "print(f\"Test acc = {best_result[1].score(X_test, y_test)}\")\n",
        "\n",
        "print(f\"w = {best_result[1].w}\")\n",
        "print(f\"b = {best_result[1].b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F81aku1YSzMc"
      },
      "source": [
        " ## Does $l1$ or $l2$ tend to produce sparser weight vectors?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_d57925NCtC"
      },
      "source": [
        "The $l1$ regularization produced a sparser weight vectors where around 11 elements are less than $10^{-5}$. In constrast, $l2$ regularization, the weight vectors have no elements approaching to zero."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:51:29) [MSC v.1929 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "eee0ec1dfc014c5e818c9fd6187bf3227a654b7ffe32fcac85496f478f59351d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
