{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DWAz39F3rVcl"
      },
      "source": [
        "# Neural Network Sequence-to-Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nTZYkqPrH8G"
      },
      "source": [
        "# Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ8eOJ_4rd-Z"
      },
      "source": [
        "We are going to implement a sequence-to-sequence model, specifically an encoder-decoder with attention, using recurrent neural networks.\n",
        "\n",
        "We will be using PyTorch again. In the last assignment, we implemented the neural network from scratch to familiarize ourselves with the bones of how they work. This time, we will be using some of the prebuilt layers in the `torch.nn` module for convenience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eko5IaF13aC"
      },
      "source": [
        "We are using the [E2E Challenge](http://www.macs.hw.ac.uk/InteractionLab/E2E/) dataset for generating restuarant descriptions from *meaning representations*.\n",
        "\n",
        "Meaning Representation:\n",
        "```\n",
        "name[The Eagle],\n",
        "eatType[coffee shop],\n",
        "food[French],\n",
        "priceRange[moderate],\n",
        "customerRating[3/5],\n",
        "area[riverside],\n",
        "kidsFriendly[yes],\n",
        "near[Burger King]\n",
        "```\n",
        "\n",
        "Generated Description:\n",
        "```\n",
        "The three star coffee shop, The Eagle, gives families a mid-priced dining experience featuring a variety of wines and cheeses. Find The Eagle near Burger King.\n",
        "```\n",
        "\n",
        "In a traditional, non-neural approach, there would be two steps in this task. First, the meaning representations would be converted into a *sentence plan* that would group and reorder them. In the example above, `customerRating`, `name`, `kidsFriendly`, `priceRange`, and `food` are grouped together, in that order, while `near` is in a separate group and `area` is omitted.\n",
        "\n",
        "The sentence plan would then be *realized* into text. The E2E challenge takes advantage of neural networks to perform this task in only one step, so-called \"end-to-end\" training. Thus the input to our neural network is a sequence of meaning representations, and the output is a sequence of words, without any intermediate sentence plan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4AlautRsXwY"
      },
      "source": [
        "# Loading and Preprocessing the Data --- 5 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb4Ldcuq1Gk4"
      },
      "source": [
        "We will use the very convenient `torchtext` module to load and process our data. \n",
        "\n",
        "The code snippet below first prompts you to upload the provided files `train.txt` and `test.txt` to Colab. Take a look at the files. They are formatted as tab-separated values. The first column, labeled \"mr\", contains a comma-separated list of meaning representations. The second column, labeled \"ref\" (short for \"reference,\" which is a term used in place of \"gold standard\" in text generation tasks), contains the sentence(s) that make up the restaurant description.\n",
        "\n",
        "Fill in the rest of this code snippet following the instructions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "Spp_Om-aJ9k0",
        "outputId": "52cb5474-be54-45ff-a9a0-fe72c134bd47"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7e87ea17-4700-4224-aa1e-c09d3f120859\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7e87ea17-4700-4224-aa1e-c09d3f120859\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving test.txt to test (1).txt\n",
            "Saving train.txt to train (1).txt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-db0ef7e4ce59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     result = _output.eval_js(\n\u001b[1;32m     71\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[0;32m---> 72\u001b[0;31m             output_id=output_id))\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'append'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0;31m# JS side uses a generator of promises to process all of the files- some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "import torchtext\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "START = '<s>'\n",
        "END = '</s>'\n",
        "MAX_LEN = 50\n",
        "\n",
        "\n",
        "# Fill in the constructor options for src and tgt to perform tokenization and preprocessing\n",
        "# You can use the global variables START, END, and MAX_LEN, as well as nltk.word_tokenize()\n",
        "src = torchtext.legacy.data.Field(\n",
        "    # Fill in option(s)\n",
        "    tokenize = lambda x: x.split(\", \")\n",
        "    )\n",
        "tgt = torchtext.legacy.data.Field(\n",
        "    # Fill in more option(s)\n",
        "    include_lengths=True,\n",
        "    init_token = START,\n",
        "    eos_token = END,\n",
        "    lower = True,\n",
        "    tokenize = nltk.word_tokenize\n",
        "    )\n",
        "\n",
        "\n",
        "train_data, test_data = torchtext.legacy.data.TabularDataset.splits(\n",
        "    path='/content/', train='train.txt', test='test.txt', format='tsv',\n",
        "    fields=[('src', src), ('tgt', tgt)],\n",
        "    filter_pred=lambda x: len(x.tgt) <= MAX_LEN,\n",
        "    skip_header=True\n",
        "    )\n",
        "\n",
        "src.build_vocab(train_data, max_size=50000)\n",
        "tgt.build_vocab(train_data, max_size=50000)\n",
        "\n",
        "print('Source and target vocabulary size')\n",
        "print(len(src.vocab.stoi))\n",
        "print(len(tgt.vocab.stoi))\n",
        "\n",
        "print('\\nSource and target vocabulary items')\n",
        "print(list(src.vocab.stoi.keys())[:10])\n",
        "print(list(tgt.vocab.stoi.keys())[:10])\n",
        "\n",
        "print('\\nNumber of training pairs: %d' % len(train_data.examples))\n",
        "print('\\nNumber of test pairs: %d' % len(test_data.examples))\n",
        "\n",
        "print('\\nTraining source and target')\n",
        "print(train_data.examples[0].src)\n",
        "print(train_data.examples[0].tgt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E78wdW7HUmn0"
      },
      "source": [
        "The code snippet instantiates two `torchtext.data.Field` objects. The first is `src`, representing the input or \"source\"; the second is `tgt`, representing the output or \"target.\" \n",
        "\n",
        "A `Field` object represents some text data that we want to be able to vectorize into `torch.Tensor`'s for a neural network. It can automatically handle the preprocessing steps, such as lowercasing and rare word replacement, which we implemented manually in the previous assignment. However, we do need to set the correct options in the `Field` constructor so that it knows what preprocessing we want to do. \n",
        "\n",
        "Some of the options in both the `src` and `tgt` constructors are already filled in. For the source/input, the only thing left to do is tokenization: each meaning representation unit is a token. For example, the string \"name[The Eagle]\" should be considered a single token.\n",
        "\n",
        "For the target/output, we also want to do tokenization, but we don't want to simply split on commas --- we want to do proper word tokenization using `nltk.word_tokenize()`. We also want to lowercase all the words and append our trusty start and end meta-tokens `'<s>'` and `'</s>'`.\n",
        "\n",
        "The code for instantiating the `torchtext.data.TabularDataset` is already completed; it will automatically convert provided files `train.txt` and `test.txt` into dataset objects. Your task is to fill in the constructors for `src` and `tgt` to perform the tokenization and preprocessing steps described above. See [the documentation here](https://torchtext.readthedocs.io/en/latest/data.html#field) for the list of options. \n",
        "\n",
        "(HINT: [Lambda expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) are very convenient here; there is an example of one in the `TabularDataset` constructor that filters out any pairs where the target sequence is too long.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxc38VoK5FET"
      },
      "source": [
        "# Building the Neural Network --- 25 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omx1fqsPYEsc"
      },
      "source": [
        "Now that we have loaded and formatted our data, it's time to build the sequence-to-sequence network. We will be using the `torch.nn` and `torch.nn.functional` modules, which implement most of the common neural network layers and activation functions that we might want to use. Instead of implementing the equations ourselves, as in the last assignment, we can simply use, for example, `torch.nn.Linear` or `torch.nn.NLLLoss`.\n",
        "\n",
        "For simplicity, we will again be using stochastic, rather than batch or mini-batch gradient descent. Batching introduces some complications for implementation, including increasing the number of dimensions in every tensor and making decoding cumbersome to code up. So our batch size throughout this assignment is simply 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BAV7fGSc5F1"
      },
      "source": [
        "## The Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOBAtESTc6Oq"
      },
      "source": [
        "This code snippet implements the encoder part of our encoder-decoder model. Recall from class that an encoder takes the input sequence and computes a sequence of hidden states. The last hidden state is used to initialize the decoder's hidden state; if we were implementing attention (which we aren't in this assignment), we would calculate it using the hidden state sequence. Fill in the encoder code snippet following the instructions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm-hMw8AYQEj",
        "outputId": "34f04717-27ff-4876-ffd3-c6ffd70bd390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder(\n",
            "  (embed): Embedding(81, 128)\n",
            "  (gru): GRU(128, 128)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "\n",
        "  # Fill in this constructor\n",
        "  # The arguments input_size and hidden_size are ints\n",
        "  # No return value\n",
        "  def __init__(self, vocab_size, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    # Fill in more lines \n",
        "    self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "\n",
        "  # Fill in this method\n",
        "  # The return type should be a torch.Tensor\n",
        "  def initialize_hidden_state(self):\n",
        "    return torch.zeros((1, 1, self.hidden_size), device='cuda')\n",
        "\n",
        "\n",
        "  # Fill in this method\n",
        "  # The arguments input_sequence and hidden_state are torch.Tensors\n",
        "  # The return type should be a torch.Tensor\n",
        "  def forward(self, input_sequence, hidden_state):\n",
        "    x = self.embed(input_sequence)\n",
        "    _, hn = self.gru(x, hidden_state)\n",
        "    return hn\n",
        "  \n",
        "  \n",
        "HIDDEN_SIZE = 128\n",
        "\n",
        "encoder = Encoder(len(src.vocab), HIDDEN_SIZE).to('cuda')\n",
        "print(encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXEFDbxJc2w8"
      },
      "source": [
        "### The Constructor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjdQ8sNQayl3"
      },
      "source": [
        "First we need to fill in the constructor. We can see that our `Encoder` is a subclass of `nn.Module`, which is PyTorch's base class for neural networks.\n",
        "\n",
        "This constructor needs to instantiate two things: the embedding layer and the recurrent layer. We are using a GRU as our recurrent unit; it gives a good tradeoff between expressiveness and computational cost.\n",
        "\n",
        "Fill in the constructor by instantiating an `nn.Embedding` and an `nn.GRU`. The constructors for both of these classes take two arguments: the input dimension size and the output dimension size. See the documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding) and [here](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU).\n",
        "\n",
        "Also notice that there is already a line in the constructor that saves `hidden_size`; we will need it in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJXXsCf1iw6h"
      },
      "source": [
        "### The Hidden State"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vENQ0ijPkm0g"
      },
      "source": [
        "Recall that a GRU calculates its hidden state like so:\n",
        "\n",
        "$\\begin{aligned}\n",
        "\\mathbf{r}_t &= \\sigma(\\mathbf{W}_r \\mathbf{x}_t + \\mathbf{U}_r \\mathbf{h}_{t-1}) \\\\\n",
        "\\mathbf{z}_t &= \\sigma(\\mathbf{W}_z \\mathbf{x}_t + \\mathbf{U}_z \\mathbf{h}_{t-1}) \\\\\n",
        "\\mathbf{n}_t &= \\text{tanh}(\\mathbf{W}_n \\mathbf{x}_t + \\mathbf{r}_t \\odot \\mathbf{U}_n \\mathbf{h}_{t-1}) \\\\\n",
        "\\mathbf{h}_t &= (1 - \\mathbf{z}_t) \\odot \\mathbf{n}_t + \\mathbf{z}_t \\odot \\mathbf{h}_{t-1}\n",
        "\\end{aligned}$\n",
        "\n",
        "All of these calculations depend on the input $x_t$ and the previous hidden state $h_{t-1}$. But wait... what happens when $t=1$? What is $h_0$?\n",
        "\n",
        "We need to initialize the hidden state. Note that the hidden state is *not* a parameter, so we don't need to use Xavier initialization like in the previous assignment; the hidden state is one of the \"outputs\" of our encoder, as we'll see when we implement the forward pass. (In case you're wondering, parameter initialization for our embedding and GRU layers was done automatically by PyTorch when you instantiated them.) It is common to simply initialize the hidden state to be all zeros. This means that at the first time step, the new hidden state $h_1$ is calculated using only the input $x_1$.\n",
        "\n",
        "Fill in the method `initialize_hidden_state()` to return a hidden state vector of the appropriate shape initialized to all zeros. Two details to keep in mind:\n",
        "- PyTorch expects an RNN hidden state to have shape `(num_layers * num_directions, batch_size, hidden_size)`; for simplicity, we are using a single direction, single layer GRU and a batch size of just 1 training example. \n",
        "- Make sure to set the option `device='cuda'` to make sure computations are done on the GPU, rather than on the CPU, which would take forever. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXjstOGursp"
      },
      "source": [
        "### The Forward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiKKFUeWuuRH"
      },
      "source": [
        "The last part of the encoder is the forward pass. As in the previous assignment, the `forward()` method should perform all of the computations needed to transform the input of our encoder into the appropriate output. Unlike in the previous assignment, though, we are using prebuilt PyTorch layers, so we don't need to implement all the equations from scratch; we can simply call `layer_name(input)` to run an input through a layer of the network.\n",
        "\n",
        "Because an encoder is an RNN-based network, the input is a sequence of words, rather than a single word like in the previous assignment. Also, a word is represented as an index in the vocabulary, rather than one-hot vector; this is basically the sparse version of a one-hot vector. Happily, we don't need to do anything special to account for these differences because PyTorch's embedding and GRU layers can handle sequential inputs and vocabulary indices automatically.\n",
        "\n",
        "Fill in the method `forward()` to first get the embeddings of the input sequence and then run that sequence of embeddings through the GRU. There are a few wrinkles here:\n",
        "\n",
        "- The input is a `torch.Tensor` of shape `(sequence_length, 1)`; the output should be the `torch.Tensor` corresponding to the final encoder hidden state after running the input through the GRU.\n",
        "\n",
        "- The initial hidden state $h_0$ is passed as an argument to the GRU, and the updated hidden state $h_t$ is returned by it. The GRU also returns an `output`, which is the sequence of hidden states $h_1, \\ldots, h_t$. If we were implementing attention, we would use this sequence, but since we're not, we can simply ignore it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9jzo9vh1vXS"
      },
      "source": [
        "## The Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7FMShXX2eS8"
      },
      "source": [
        "Now let's implement the decoder part of our encoder-decoder model in the code snippet below. Notice that unlike with the encoder, we don't need an `initialize_hidden_state()` method because the decoder's hidden state is initialized with the last hidden state of the encoder. Fill in the snippet following the instructions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eqo0_EiY20OL",
        "outputId": "a391b7f0-8197-429d-ca55-75b9630e0b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder(\n",
            "  (embed): Embedding(2758, 128)\n",
            "  (gru): GRU(128, 128)\n",
            "  (out): Linear(in_features=128, out_features=2758, bias=True)\n",
            "  (softmax): LogSoftmax(dim=2)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Decoder(nn.Module):\n",
        "  \n",
        "\n",
        "  # Fill in this constructor\n",
        "  # The arguments hidden_size and vocab_size are ints\n",
        "  # No return type\n",
        "  def __init__(self, hidden_size, vocab_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    # Fill in more lines\n",
        "    self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    self.out = nn.Linear(hidden_size, vocab_size)\n",
        "    self.softmax = nn.LogSoftmax(2)\n",
        "\n",
        "\n",
        "  # Fill in this method\n",
        "  # The arguments prev_word and hidden_state are torch.Tensors\n",
        "  # The return type should be a tuple (output, hidden_state), which are both torch.Tensors\n",
        "  def forward(self, prev_word, hidden_state):\n",
        "    x = self.embed(prev_word)\n",
        "    _, hn = self.gru(x, hidden_state)\n",
        "    output = self.softmax(self.out(hn))\n",
        "    return (output, hn)\n",
        "\n",
        "\n",
        "decoder = Decoder(HIDDEN_SIZE, len(tgt.vocab)).to('cuda')\n",
        "print(decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdTMwEEdAgAy"
      },
      "source": [
        "### The Constructor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzbuGiuFAi8D"
      },
      "source": [
        "As with the encoder, our decoder is a subclass of `nn.Module`. In addition to the the embedding layer and the GRU layer, we need to instantiate two other layers: an output layer and a (log) softmax \"layer.\"\n",
        "\n",
        "Fill in the constructor by instantiating an `nn.Embedding`, an `nn.GRU`, an `nn.Linear` (the output layer), and an `nn.LogSoftmax` (the (log) softmax \"layer\").\n",
        "\n",
        "For the `nn.LogSoftmax` \"layer\", there is only one argument to the constructor: which dimension to compute the softmax over, ie. the values along that dimension will be squashed into the range $[0, 1]$ and all sum to 1. Make sure you fill in the correct dimension! If you put the wrong dimension, the code will still run without errors, but your model will not work correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2t9qphxCMFF"
      },
      "source": [
        "### The Forward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFyjRZpsCLZR"
      },
      "source": [
        "Similarly to the encoder, the decoder's forward pass should run the input through the embedding layer, the GRU layer, the output layer, and the (log) softmax layer; the return value should be a (log) probability distribution over the vocabulary. Fill in the `forward()` method above. The key differences from the encoder's method are as follows:\n",
        "\n",
        "- Because autoregressive generation occurs one word at a time, the decoder runs on a single word at a time, rather than a sequence of words. Thus the decoder takes the index of a single word, ie. the previous word from the output sequence, as input. As we discussed in class, this can either be the gold standard previous word $y_{i-1}$ or the model's own prediction $\\hat{y}_{i-1}$.\n",
        "\n",
        "- We can still ignore the `output` of the GRU layer; since we are running a single word at a time, `output` is exactly the same as the hidden state.\n",
        "\n",
        "- We want to return both the output (log) probability distribution and the updated hidden state. The output distribution allows us to predict a word for this time step, while the updated hidden state will be passed to the decoder at the next time step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KeLDdyuEF8D"
      },
      "source": [
        "# Training the Neural Network --- 15 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ve4vMBoGtM6"
      },
      "source": [
        "Now that we've built the network, it's time to train! By convention, and to keep code organized, PyTorch training code is often broken into two functions, one that runs a single training example (or mini-batch) through the network, computes the loss, and performs an update, and a second that manages loading the training data and looping through the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKxcN5-kHd7Q"
      },
      "source": [
        "## Training on a Single Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1Z-hJXFHhf_"
      },
      "source": [
        "First we'll write the function to run a single training pair, consisting of one input sequence and one output sequence, through our encoder-decoder network.\n",
        "\n",
        "You'll need to fill in the code snippet following the instructions below.\n",
        "\n",
        "But first, let's talk about some of the arguments.\n",
        "\n",
        "- `input_sequence` and `target_sequence`. These are `torch.Tensor`'s of shape `(sequence_length, 1)`, ie. they are matrices where each row represents a meaning representation or word and contains a single value, which is the index of that word in the corresponding source (input) or target vocabulary.\n",
        "\n",
        "- `encoder_optimizer` and `decoder_optimizer`. These are instances of optimizer classes from the module `torch.optim`, which implement many of the adjustable learning rate algorithms we saw in class. For example, there are `torch.optim.SGD`, `torch.optim.Adagrad`, and `torch.optim.RMSProp`.\n",
        "\n",
        "- `loss_function` is a class from the `torch.nn` module that implements a loss function. For example, there are `torch.nn.CrossEntropyLoss` and `torch.nn.CosineEmbeddingLoss`.\n",
        "\n",
        "The first step in the `train()` function, resetting the computational graphs of `encoder_optimizer` and `decoder_optimizer` using `zero_grad()`, has already been completed. The last steps and return statement have also been completed. Your task is to fill in the sections before and after the statement `loss = 0` to finish the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjKZ32d3Hi4v"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Fill in this function\n",
        "# The arguments input_sequence and target_sequence are torch.Tensors\n",
        "# The argument target_length is an int\n",
        "# The argument tgt_vocab is a torchtext.vocab.Vocab object\n",
        "# The arguments encoder and decoder are our Encoder and Decoder\n",
        "# The arguments encoder_optimizer and decoder_optimizer are torch.optim.Optimizers\n",
        "# The argument loss_function is a torch.nn loss function class\n",
        "# The argument teacher_forcing_ratio is a float\n",
        "# The return type should be a float\n",
        "def train(input_sequence, target_sequence, target_length, tgt_vocab,\n",
        "          encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "          loss_function, teacher_forcing_ratio=0.5):\n",
        "  \n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "  # Fill in the encoder part and setting up for the decoder part here\n",
        "  tgt_dict = tgt_vocab.stoi\n",
        "  h0 = encoder.initialize_hidden_state()\n",
        "  hidden_state = encoder.forward(input_sequence, h0)\n",
        "  prev_word = torch.tensor([[tgt_dict[START]]], device='cuda')\n",
        "\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "\n",
        "  # Fill in the decoder loop here\n",
        "\n",
        "  for i in range(1, target_length):\n",
        "    probs, hidden_state = decoder.forward(prev_word, hidden_state)\n",
        "    loss += loss_function(torch.squeeze(probs, 0), target_sequence[i]) \n",
        "    if random.uniform(0, 1) >= teacher_forcing_ratio:\n",
        "      # Use predicted y\n",
        "      prev_word = torch.squeeze(torch.topk(probs, 1, 2).indices.detach(), 0)\n",
        "    else:\n",
        "      # Use gold standard y \n",
        "      prev_word = torch.tensor([[target_sequence[i]]], device='cuda')\n",
        "\n",
        "\n",
        "\n",
        "  loss.backward()\n",
        "  encoder_optimizer.step()\n",
        "  decoder_optimizer.step()\n",
        "  return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk7dAwWjdG6w"
      },
      "source": [
        "### Running the Encoder and Setting Up the Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL4sdEdVPotu"
      },
      "source": [
        "\n",
        "\n",
        "You can fill in the middle of this function following these steps:\n",
        "\n",
        "1. Initialize the encoder hidden state and run `input_sequence` through the encoder.\n",
        "\n",
        "2. Set the decoder hidden state equal to the updated encoder hidden state.\n",
        "\n",
        "3. Initialize the first decoder `prev_word` $y_0$ to be the start meta-token. Recall that a word is represented not by a one-hot vector, as in the previous assignment, but by its index in the vocabulary. Thus `prev_word` should be a `torch.Tensor` containing a single value, the index of the start meta-token in the target vocabulary. You can look up the index corresponding to a given word using the dictionary `tgt_vocab.stoi` (standing for \"string to index\"), which is provided by `torchtext`. Make sure you set the option `device='cuda'` as well so that PyTorch puts this tensor on the GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DwKzZFqm9Yb"
      },
      "source": [
        "**WARNING:** Be very careful with the shape of `prev_word`. Look back at the code for `Encoder` and `Decoder` if you're not sure what the shape should be. If the shape is not correct, you will get an error when you try to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS67dCk0dPwL"
      },
      "source": [
        "### Running the Decoder Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9ScifCVW00Y"
      },
      "source": [
        "Now everything is set up to run the decoder. Recall that we will run the decoder on one `prev_word` at a time. It will output a (log) probability distribution over the vocabulary, which we can use to generate the predicted word $\\hat{y}_i$. So we want to implement a loop that does the following:\n",
        "\n",
        "1. Run the `prev_word` $y_{i-1}$ through the decoder to get both the (log) probability distribution and the updated decoder hidden state $h_i$.\n",
        "\n",
        "2. Compute the loss using `loss_function()`. A call to `loss_function()` takes two `torch.Tensor` arguments: the output of the decoder and the gold standard target word $y_i$ from `target_sequence`. Add this value to the variable `loss`.\n",
        "\n",
        "3. Decide if you are using teacher forcing ($y_i$) or not ($\\hat{y}_i$) for the next time step. The argument `teacher_forcing_ratio` tells you the probability that a given decoder step uses teacher forcing. You can use the `random` module to make this decision.\n",
        "\n",
        "4. If using teacher forcing, the new `prev_word` should be the gold standard target word. If not, then the new `prev_word` is the predicted word. At training time, the predicted word is simply the word with the highest output probability (ie.  greedy decoding). You can get this word using `torch.topk()` (see [the documentation here](https://pytorch.org/docs/stable/generated/torch.topk.html)). You will also have to call `detach()` to unlink it from the computation graph that generated this output, otherwise the computation graph for the current time step $i$ will get connected to the new computation graph for the next time step $i+1$, which will blow up our training time because we now need to do backpropagation through longer and longer graphs the more words we predict. This detach step is part of the truncated backpropagation through time, which we discussed in class.\n",
        "\n",
        "5. Repeat the loop with the new `prev_word` and updated decoder hidden state until you have predicted a word for every position in the target sequence.\n",
        "\n",
        "After this loop runs, the last lines that have already been completed will execute: running the backward pass using `loss.backward()`, performing parameter updates using `optimizer.step()`, and returning the average word-level loss for this training pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKkfWkHQb77l"
      },
      "source": [
        "**WARNING:** Far and away the most common source of error when implementing a neural network is incorrect tensor shape. Be very careful when creating a new tensor, eg. the first `prev_word`, or when slicing an existing tensor, eg. to get $y_i$ from `target_sequence`, that the shape of your tensor matches what the rest of the network expects. Carefully read the input and output tensor shapes of the PyTorch embedding and GRU layers. You can use the functions `torch.unsqueeze()` and `torch.squeeze()` to add or get rid of extra dimensions in your tensors (see the documentation [here](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html) and [here](https://pytorch.org/docs/stable/generated/torch.squeeze.html)). When in doubt, print `Tensor.size()` to see if the shape makes sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaxBJEz65Isa"
      },
      "source": [
        "## Training an Epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZRRl4Fo-vhg"
      },
      "source": [
        "The code snippet below is already complete; you don't need to fill in anything. It initializes our encoder and decoder optimizers (we are using vanilla gradient descent), as well as the loss function, negative log likelihood loss. It then loads the training data using a `torchtext.data.Iterator` object and runs each training pair through our network using the `train()` function we just wrote."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxqfBc2H5RGb"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "# Run training epochs and report the average loss per epoch\n",
        "def train_epochs(train_data, tgt_vocab, encoder, decoder,\n",
        "                num_epochs=1,\n",
        "                learning_rate=0.01, teacher_forcing_ratio=0.5):\n",
        "  encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "  decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "  loss_function = nn.NLLLoss()\n",
        "\n",
        "  data_generator = torchtext.legacy.data.Iterator(\n",
        "      dataset=train_data,\n",
        "      batch_size=1,\n",
        "      device='cuda',\n",
        "  )\n",
        "\n",
        "  for e in range(num_epochs):\n",
        "    print('Epoch %d' %e)\n",
        "    count, epoch_loss = 0, 0\n",
        "    for training_example in data_generator.__iter__():\n",
        "      input_sequence = getattr(training_example, 'src')\n",
        "      target_sequence, target_length = getattr(training_example, 'tgt')\n",
        "\n",
        "      epoch_loss += train(input_sequence, target_sequence, target_length, tgt_vocab,\n",
        "                          encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "                          loss_function, teacher_forcing_ratio)\n",
        "      count += 1\n",
        "    print('Loss %f' % (epoch_loss.item() / count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lOu2I29_NB_"
      },
      "source": [
        "We are now ready to train the model using the code snippet below. We are running on the GPU this time, so training is much faster; one epoch should take around 15-25 minutes. If you want to run multiple epochs at a time, feel free to increase `num_epochs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "OtreZZqG_PNm",
        "outputId": "0bbc6870-0578-447f-88c2-ef297db59037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-12-03 17:14:30.509785\n",
            "Epoch 0\n",
            "Loss 2.599473\n",
            "2021-12-03 17:33:37.563766\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_ac40e6da-8699-4a63-9d32-b103aa631d51\", \"hw5.encoder\", 439079)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_e7decf41-f2d8-4249-bfad-868cff0f478a\", \"hw5.decoder\", 3233247)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "print(datetime.datetime.now())\n",
        "train_epochs(train_data, tgt.vocab, encoder, decoder, num_epochs=1)\n",
        "print(datetime.datetime.now())\n",
        "\n",
        "# Auto save after done\n",
        "\n",
        "# torch.save(encoder.state_dict(), 'hw5.encoder')\n",
        "# torch.save(decoder.state_dict(), 'hw5.decoder')\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download('hw5.encoder')\n",
        "# files.download('hw5.decoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6wwLPNBllMV"
      },
      "source": [
        "**WARNING:** Error messages tend to be unhelpful when running on the GPU. This is partly because the GPU executes operations in parallel, and it can also execute asynchronously from the CPU. As a result, you often can't tell which line of code is actually causing the error; the stack trace can be out of sync with GPU execution. Another reason is that, once a tensor is loaded on the GPU, its dimension and/or indexing errors become GPU memory access errors, and memory errors of any kind are difficult to debug. \n",
        "\n",
        "If you are still in the process of debugging your code, it is much easier to do so on the CPU. You can run on the CPU by removing the `device='cuda'` options and `to('cuda')` calls. Once your code runs successfully on the CPU, you can switch back to the GPU.\n",
        "\n",
        "If you do get a GPU memory error (ie. it is a runtime error with `cuda`, `cublas`, or `cudnn` in the name), you will need to reset your Colab runtime using the Runtime menu to clear the GPU memory before you can run again. Otherwise the GPU memory will remain in its errorful state, and you will constantly get weird error messages, even on code snippets that previously worked fine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkeYyf0j_sx_"
      },
      "source": [
        "Once you've successfully trained, you can use the code snippets below for saving and loading the model so that you can save your progress and also continue training more epochs as time allows. Notice that we only save/load `state_dict`, which is the dictionary where PyTorch stores the current parameter values of a model. We can easily instantiate a new encoder/decoder and overwrite its randomly initialized parameters with our saved values, so saving only the parameters is more space-efficient than saving the entire network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Jk1Cpna4_09Q",
        "outputId": "de9c4335-9741-4d10-8cc9-f89ce1c7819b"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_b7861a7d-871c-4ab0-92d3-86627a58c559\", \"hw5.encoder\", 439224)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_61822397-0f29-4e2b-8d41-d9487854a0d1\", \"hw5.decoder\", 3233602)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Save and download the trained model parameters\n",
        "\n",
        "torch.save(encoder.state_dict(), 'hw5.encoder')\n",
        "torch.save(decoder.state_dict(), 'hw5.decoder')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('hw5.encoder')\n",
        "files.download('hw5.decoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "srrVKR2VBgM-",
        "outputId": "e55d77f3-b60c-4d59-ba41-fec0154909b0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3254d795-376c-497e-a8ab-a32766e0ecd8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3254d795-376c-497e-a8ab-a32766e0ecd8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving hw5.decoder to hw5 (1).decoder\n",
            "Saving hw5.encoder to hw5 (1).encoder\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Upload saved model parameters\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "encoder.load_state_dict(torch.load('/content/hw5.encoder'))\n",
        "decoder.load_state_dict(torch.load('/content/hw5.decoder'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGKJTGYQ9TMB"
      },
      "source": [
        "# Decoding --- 5 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBtkE-8TDeFv"
      },
      "source": [
        "Now that we have a trained model, we can generate some restaurant descriptions! Fill in the function below to generate a resturant description from an input list of meaning representations (strings). As before, you want to run the input through the encoder, then set up the decoder and run the decoder loop, generating the highest (log) probability word at each iteration (ie. greedy decoding); you should be able to reuse some of your code from `train()`. The key differences from `train()` are as follows:\n",
        "\n",
        "- You should wrap the entire contents of this function with a `with` statement that sets `torch.no_grad()`. This will tell PyTorch that it doesn't need to keep track of the computation graph values and gradients. Since we aren't doing parameter updates anymore, we don't need the gradients, so telling PyTorch to skip calculating them will make this function run faster.\n",
        "\n",
        "- In the decoder loop, you should stop generating words and return if the end meta-token is generated before `max_length` is reached; otherwise, you should generate `max_length` words and then return. Don't include the start meta-token or the end meta-token, if generated, in the returned list of words.\n",
        "\n",
        "- The return type of this function is a list of strings, but decoder predicts a tensor containing an index into the target vocabulary. You will need to first convert this tensor into an int using `Tensor.item()`, and then convert the int into a word using the `torchtext` dictionary `tgt_vocab.itos` (\"index to string\").\n",
        "\n",
        "As in `train()`, be very careful with the shapes of your tensors. Most errors in neural network implementation are due to tensor shape mismatch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx88ZOhjEZvn",
        "outputId": "ccfc6386-2ac9-4e7b-af23-d89edef0bb2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['name[Blue Spice]', 'priceRange[high]', 'customer rating[1 out of 5]', 'area[riverside]', 'familyFriendly[yes]']\n",
            "blue spice is a riverside area , is a child friendly restaurant with a customer out of 5 customer rating .\n",
            "\n",
            "['name[Loch Fyne]', 'food[French]', 'customer rating[low]', 'area[riverside]', 'near[The Rice Boat]']\n",
            "loch fyne is a french restaurant located near the rice boat in the riverside area .\n",
            "\n",
            "['name[The Eagle]', 'eatType[coffee shop]', 'food[Indian]', 'priceRange[moderate]', 'customer rating[3 out of 5]', 'area[riverside]', 'familyFriendly[yes]', 'near[Burger King]']\n",
            "the eagle is a coffee shop serving indian food at a moderate price range . it is located in the riverside area near burger king .\n",
            "\n",
            "['name[Fitzbillies]', 'priceRange[cheap]', 'customer rating[5 out of 5]', 'familyFriendly[no]', 'near[Express by Holiday Inn]']\n",
            "fitzbillies is a cheap restaurant near express by holiday inn .\n",
            "\n",
            "['name[Wildwood]', 'eatType[pub]', 'food[Indian]', 'priceRange[less than 20]', 'customer rating[low]']\n",
            "wildwood is a pub that indian food with a price range less than 20 . it customer rating is low .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fill in this function\n",
        "# The argument input_sequence is a torch.Tensor\n",
        "# The arguments src_vocab and tgt_vocab are torchtext.vocab.Vocab objects\n",
        "# The arguments encoder and decoder are our Encoder and Decoder\n",
        "# The argument max_length is an int\n",
        "# The return type should be a list of strings\n",
        "def generate(input_sequence, src_vocab, tgt_vocab, encoder, decoder, max_length=MAX_LEN):\n",
        "  with torch.no_grad():\n",
        "    result = []\n",
        "\n",
        "    # Fill in the encoder part and setting up for the decoder part here\n",
        "    tgt_dict = tgt_vocab.stoi\n",
        "    h0 = encoder.initialize_hidden_state()\n",
        "    hidden_state = encoder.forward(input_sequence, h0)\n",
        "    prev_word = torch.tensor([[tgt_dict[START]]], device='cuda')\n",
        "\n",
        "    # Fill in the decoder loop here\n",
        "    for i in range(MAX_LEN):\n",
        "      probs, hidden_state = decoder.forward(prev_word, hidden_state)\n",
        "      prev_word = torch.squeeze(torch.topk(probs, 1, 2).indices.detach(), 0)\n",
        "      word_string = tgt_vocab.itos[prev_word.item()]\n",
        "      if word_string == \"</s>\":\n",
        "        break\n",
        "      else:\n",
        "        result.append(word_string)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    input = random.choice(train_data.examples).src\n",
        "    print(input)\n",
        "    input_sequence = torch.tensor([src.vocab.stoi[word] for word in input], device='cuda').unsqueeze(1)\n",
        "    print(' '.join(generate(input_sequence, src.vocab, tgt.vocab, encoder, decoder)))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29yQdDz5RXwZ"
      },
      "source": [
        "You should get some fairly okay results on the training set after a single epoch, and the fluency of the generated descriptions will improve with subsequent epochs of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ-llrxaSmxd"
      },
      "source": [
        "# Evaluation --- 5 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OdXAy2BSsBO"
      },
      "source": [
        "The last thing we need to do is evaluate the performance of the model. We will use the BLEU metric, which is commonly used for translation-type tasks, like this one. The formula for BLEU is \n",
        "\n",
        "$\\text{min} \\left( 1, \\cfrac{\\text{generation length}}{\\text{target length}} \\right) \\left( \\displaystyle{\\prod_{i=1}^4} \\text{precision}_i \\right)^{\\frac{1}{4}}$\n",
        "\n",
        "where $\\text{precision}_i$ refers to the n-gram precision with $n=i$.\n",
        "\n",
        "For these n-grams, we won't add any meta-tokens to pad the beginning and end of the sentences; we only count the words that actually occur in our generated sentence and the target sentence. \n",
        "\n",
        "Fill in the code snippet below to implement BLEU, following the formula above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m5vcCG8eMdS"
      },
      "outputs": [],
      "source": [
        "# Fill in this function\n",
        "# The arguments generated_words and target_words are lists of strings\n",
        "# The return type should be a float\n",
        "def bleu(generated_words, target_words, n=4):\n",
        "  brevity = min(1, len(generated_words) / len(target_words))\n",
        "\n",
        "  # Calculates regular precision (not clipped/modified)\n",
        "  avg_precision = 1\n",
        "  for i in range (1, n + 1):\n",
        "    count = 0\n",
        "    # create list of ngrams where n = i\n",
        "    target_ngrams = list(zip(*[target_words[j:] for j in range(i)]))\n",
        "    generated_ngrams = list(zip(*[generated_words[j:] for j in range(i)]))\n",
        "    length_generated = len(generated_ngrams)\n",
        "\n",
        "    for ngrams in generated_ngrams:\n",
        "      if ngrams in target_ngrams:\n",
        "        count += 1\n",
        "\n",
        "    avg_precision *= ((count / length_generated) ** (1/n))\n",
        "  \n",
        "  return brevity * avg_precision\n",
        "  # return nltk.translate.bleu_score.sentence_bleu(target_words, generated_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAd_LQ1YWQck"
      },
      "source": [
        "This last code snippet will calculate the average BLEU score of your model across the entire test set. You should get okay performance after just one epoch, and your score will increase with more training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f41eYg4YACeM",
        "outputId": "0a434799-e505-489d-d56a-633e983cf0b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.19985991924269392\n"
          ]
        }
      ],
      "source": [
        "# Calculate average BLEU over the test set\n",
        "\n",
        "num_test = len(test_data.examples)\n",
        "total_bleu = 0.\n",
        "\n",
        "for test_example in test_data.examples:\n",
        "  input, target = test_example.src, test_example.tgt\n",
        "  input_sequence = torch.tensor([src.vocab.stoi[word] for word in input], device='cuda').unsqueeze(1)\n",
        "  total_bleu += bleu(generate(input_sequence, src.vocab, tgt.vocab, encoder, decoder), target)\n",
        "\n",
        "print(total_bleu / num_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T2TO1OSWf8P"
      },
      "source": [
        "All done! Make sure your trained model parameters are saved, and use the \"File\" menu to download this notebook for GradeScope submission."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HW5.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:51:29) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "eee0ec1dfc014c5e818c9fd6187bf3227a654b7ffe32fcac85496f478f59351d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
